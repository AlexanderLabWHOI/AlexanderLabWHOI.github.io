[{"authors":["admin"],"categories":null,"content":"Our lab focuses on the application of computational approaches to oceanographic questions. We use a combination of culture- and field-based studies to better understand the biogeochemical functioning and physiological ecology of eukaryotic plankton in a changing ocean. Topics of interest include the maintenance of genetic diversity in planktonic populations and the role of biodiversity in ecosystem functioning.\nWe aim to make research done in this lab openly available and reproducible, and work to share these approaches with other researchers.\nInterested in joining the lab?\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://alexanderlabwhoi.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"Our lab focuses on the application of computational approaches to oceanographic questions. We use a combination of culture- and field-based studies to better understand the biogeochemical functioning and physiological ecology of eukaryotic plankton in a changing ocean. Topics of interest include the maintenance of genetic diversity in planktonic populations and the role of biodiversity in ecosystem functioning.\nWe aim to make research done in this lab openly available and reproducible, and work to share these approaches with other researchers.","tags":null,"title":"Alexander Lab","type":"author"},{"authors":["akrinos"],"categories":null,"content":"Arianna Krinos is a graduate student in the MIT-WHOI Joint Program in Biological Oceanography. She graduated from Virginia Tech in the fields of computer science and biology in May 2019. Past research experiences include amphibian skin microbiology for community classification and disease ecology, modeling of potential harmful algal blooms in lake ecosystems using a 1-D hydrodynamic model, MCMC methods for modeling atmospheric carbon dioxide concentrations, the physiology of lobster calcification, and individual based modeling of blue crabs in the Chesapeake Bay.\nArianna works at the interface of modeling, bioinformatics, and experimental/field work in the Alexander Lab and with the Darwin Project at MIT. In particular, she conducts laboratory culture experiments on calcifying phytoplankter Emiliania huxleyi and explores community ecology via metatranscriptomics. Arianna is excited about questions related to microbial diversity, acclimation and adaptation, and global change. She is interested in education and computer/data science literacy.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"0b410bcb784a4d3bdd48bc5f5ff4089c","permalink":"https://alexanderlabwhoi.github.io/authors/akrinos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/akrinos/","section":"author","summary":"Arianna Krinos is a graduate student in the MIT-WHOI Joint Program in Biological Oceanography. She graduated from Virginia Tech in the fields of computer science and biology in May 2019. Past research experiences include amphibian skin microbiology for community classification and disease ecology, modeling of potential harmful algal blooms in lake ecosystems using a 1-D hydrodynamic model, MCMC methods for modeling atmospheric carbon dioxide concentrations, the physiology of lobster calcification, and individual based modeling of blue crabs in the Chesapeake Bay.","tags":null,"title":"Arianna Krinos","type":"author"},{"authors":["cnobrega"],"categories":null,"content":"Celeste Nobrega is an undergraduate student in the class of 2022 at Wheaton College. She is working towards a degree in Bioinformatics, and is excited about her coursework including genomics, biology, ecology, statistics, and computer science. Past research experiences in 2020 include studying the effects of chronic cortisol exposure in zebrafish embryos where she focused my project on the response in gene expression to different cortisol treatments and collection times. In 2019, she worked on a software development and research team for a webtool called Lexos.\nCeleste is excited about the learning opportunities found at the intersection of bioinformatics and oceanography in Dr. Alexander’s lab. She is looking forward to gaining further experience in large scale genomic data and the applications of bioinformatic methods in oceanographic studies.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"a8bea7543b4ac2cf933125da0c8256db","permalink":"https://alexanderlabwhoi.github.io/authors/cnobrega/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/cnobrega/","section":"author","summary":"Celeste Nobrega is an undergraduate student in the class of 2022 at Wheaton College. She is working towards a degree in Bioinformatics, and is excited about her coursework including genomics, biology, ecology, statistics, and computer science. Past research experiences in 2020 include studying the effects of chronic cortisol exposure in zebrafish embryos where she focused my project on the response in gene expression to different cortisol treatments and collection times. In 2019, she worked on a software development and research team for a webtool called Lexos.","tags":null,"title":"Celeste Nobrega","type":"author"},{"authors":["halexand"],"categories":null,"content":"Harriet Alexander is an Assistant Scientist in the Biology Department at the Woods Hole Oceanographic Institution. Her lab works to better characterize the interplay of physiological ecology and diversity in the ocean with a focus on eukaryotic plankton. Much of her research has leveraged meta-omic data to derive insights into the diversity and function of these ecosystems.\nBefore starting at a lab at WHOI in 2018, Harriet earned her Ph.D. in Biological Oceanography from the MIT-WHOI Joint Program with Sonya Dyhrman in 2016 and moved to University of California Davis where she worked as a postdoc in the Data Intensive Biology Lab with Titus Brown from 2016-2018.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"51cf5bc3e2fa2600366989869ba8fcfd","permalink":"https://alexanderlabwhoi.github.io/authors/halexand/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/halexand/","section":"author","summary":"Harriet Alexander is an Assistant Scientist in the Biology Department at the Woods Hole Oceanographic Institution. Her lab works to better characterize the interplay of physiological ecology and diversity in the ocean with a focus on eukaryotic plankton. Much of her research has leveraged meta-omic data to derive insights into the diversity and function of these ecosystems.\nBefore starting at a lab at WHOI in 2018, Harriet earned her Ph.D. in Biological Oceanography from the MIT-WHOI Joint Program with Sonya Dyhrman in 2016 and moved to University of California Davis where she worked as a postdoc in the Data Intensive Biology Lab with Titus Brown from 2016-2018.","tags":null,"title":"Harriet Alexander","type":"author"},{"authors":["krice"],"categories":null,"content":"Kevin Rice is a senior undergraduate at the University of Nebraska-Lincoln, studying systems biochemistry. He has a particular interest in microbial ecology, bioinformatics, biological oceanography and ecological modeling. His previous research projects include understanding the function of rare taxa in the Daphnia magna microbiome, as well as quantifying bacterial DMSP synthesis potential.\nKevin is looking forward to learning more about oceanography and genomics in the Alexander lab!\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2770d50e92917b1a7547ae755de47662","permalink":"https://alexanderlabwhoi.github.io/authors/krice/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/krice/","section":"author","summary":"Kevin Rice is a senior undergraduate at the University of Nebraska-Lincoln, studying systems biochemistry. He has a particular interest in microbial ecology, bioinformatics, biological oceanography and ecological modeling. His previous research projects include understanding the function of rare taxa in the Daphnia magna microbiome, as well as quantifying bacterial DMSP synthesis potential.\nKevin is looking forward to learning more about oceanography and genomics in the Alexander lab!","tags":null,"title":"Kevin Rice","type":"author"},{"authors":["lblum"],"categories":null,"content":"Laura Blum was an SSF with Harriet Alexander and Maria Pachiadaki and did work looking at the biogeography of nitrogen fixation in particle associated communities. After finishing her undergraduate in Biolog at Middlebury College she began a Masters in Earth Sciences at Dartmouth College in Fall 2020. In the past she has enjoyed research on developing metabarcoding methods for detecting invasive aquatic plants as well as characterizing reactive oxygen species dynamics on coral reefs. She plans to keep studying amazing microbes and to explore the field of origin of life research in the future.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d704cce7836738aaece29fd6a42eaf4a","permalink":"https://alexanderlabwhoi.github.io/authors/lblum/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/lblum/","section":"author","summary":"Laura Blum was an SSF with Harriet Alexander and Maria Pachiadaki and did work looking at the biogeography of nitrogen fixation in particle associated communities. After finishing her undergraduate in Biolog at Middlebury College she began a Masters in Earth Sciences at Dartmouth College in Fall 2020. In the past she has enjoyed research on developing metabarcoding methods for detecting invasive aquatic plants as well as characterizing reactive oxygen species dynamics on coral reefs.","tags":null,"title":"Laura Blum","type":"author"},{"authors":["mmarsbris"],"categories":null,"content":"Maggi Brisbin is a postdoc at WHOI. She recently completed her PhD at the Okinawa Institute of Science and Technology in Okinawa, Japan. Her thesis work focused on the symbiotic relationship between the haptophyte algae, Phaeocystis, and acantharian hosts. Maggi is particularly interested in how interactions between the smallest organisms can affect large scale processes in the ocean. At WHOI she will continue to study microbial interactions and will focus on how bacteria might influence the ecological success of different Phaeocystis species in varying environmental conditions.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"5a937d06072f9ec633021d977d37f216","permalink":"https://alexanderlabwhoi.github.io/authors/mmarsbris/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mmarsbris/","section":"author","summary":"Maggi Brisbin is a postdoc at WHOI. She recently completed her PhD at the Okinawa Institute of Science and Technology in Okinawa, Japan. Her thesis work focused on the symbiotic relationship between the haptophyte algae, Phaeocystis, and acantharian hosts. Maggi is particularly interested in how interactions between the smallest organisms can affect large scale processes in the ocean. At WHOI she will continue to study microbial interactions and will focus on how bacteria might influence the ecological success of different Phaeocystis species in varying environmental conditions.","tags":null,"title":"Margaret Mars Brisbin","type":"author"},{"authors":["shu"],"categories":null,"content":"Sarah is a Postdoctoral Fellow at WHOI investigating the ecological roles microbial eukaryotes play in marine ecosystems. She is currently focused on characterizing the microbial eukaryotic communities found at deep sea hydrothermal vents to understand their place in marine food webs in the deep biosphere.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d13e9631611f9d00ab18e7ec6f3420c2","permalink":"https://alexanderlabwhoi.github.io/authors/shu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shu/","section":"author","summary":"Sarah is a Postdoctoral Fellow at WHOI investigating the ecological roles microbial eukaryotes play in marine ecosystems. She is currently focused on characterizing the microbial eukaryotic communities found at deep sea hydrothermal vents to understand their place in marine food webs in the deep biosphere.","tags":null,"title":"Sarah Hu","type":"author"},{"authors":["Arianna Krinos"],"categories":["computation","data management"],"content":"RStudio was once my favorite and most often-used IDE. And there are still a lot of reasons to use it, even if you\u0026rsquo;ve gotten used to something like Spyder or Jupyter. RStudio is a clean IDE that allows you to simultaneously view your filesystem, environment variables, R scripts, and console, and is all very specifically tailored to R, which is important in many cases (R is an amazing but finicky language). There are many occasions when doing \u0026ldquo;big data\u0026rdquo;-esque work on an HPC in which you may need access to an IDE. The perhaps most frequent situations are visualizing/exploring data and debugging.\nFor this tutorial, you\u0026rsquo;ll need a few things. The first thing is the most important, and it\u0026rsquo;s a little bit out of your hands. Your HPC system needs to be set up for X11 forwarding, which is a way to access the graphical window being spun up by the remote system (with access to all the files on that system) on your own computer. So we\u0026rsquo;re taking the visual output that the HPC can generate but doesn\u0026rsquo;t have the display to visualize, and sending it somewhere on our own computer. Namely, we\u0026rsquo;ll use XQuartz to visualize this incoming data stream, which is the second prerequisite.\nThe X11-related software you\u0026rsquo;ll need on your computer differs based on whether you are a Mac or a PC. This website contains links to both. If you\u0026rsquo;re on Mac, you\u0026rsquo;ll need to download XQuartz, and if you\u0026rsquo;re on PC, it\u0026rsquo;ll be Xming.\nThe third prerequisite is that you need RStudio installed somehow on your HPC system. My preferred method of doing this is via conda. While we\u0026rsquo;re at it, we should also install some dependencies that will come up in your use of R. Once you have conda set up, you can use the following command to set up an R environment that should work:\nconda create -n myRenv -c r r-essentials r-base rstudio r-lattice  Okay, now that we\u0026rsquo;re through the prerequisites, we can start setting up an RStudio window on our remote HPC. First, try logging into your remote HPC with XQuartz enabled, like so:\nssh -v -X \u0026lt;your username\u0026gt;@\u0026lt;your HPC address\u0026gt;  If you\u0026rsquo;re not used to using the -v (verbose) flag when you log in, this will be an unwelcome spam to your screen. But the reason I suggest this is that you can sift through all of that noise to look for a line that looks something like this:\ndebug1: Requesting X11 forwarding with authentication spoofing.  So that you know your HPC system indeed is set up for X11 forwarding and isn\u0026rsquo;t just ignoring the flag you offered it.\nNext, we want to set up a compute or interactive node that we can use for long enough to get our work in RStudio finished. This may differ system-to-system. If you\u0026rsquo;re using SLURM, you can use a command like this:\nsrun --time=\u0026lt;some time limit\u0026gt; -p \u0026lt;queue to use\u0026gt; --mem=\u0026lt;some memory requirement\u0026gt; --pty bash  Once you\u0026rsquo;re allocated resources, you should see a prompt that displays the new node you\u0026rsquo;ve been redirected to. Next, you\u0026rsquo;ll type exit into the window to leave this node and return to your original node (most likely the login node). This is because we need to log into this new node that we reserved with that -X flag, in order for it to recognize our request to use X11 forwarding. So, remembering the node that you were dispatched to, type:\nssh -X \u0026lt;your user\u0026gt;@\u0026lt;your new node\u0026gt;  (Depending on your present location, you might not need the username). Now, we can activate that conda environment that we prepared before getting started.\nconda activate myRenv  Which should make RStudio available to you. Now, we\u0026rsquo;re ready to launch RStudio! Your XQuartz or Xming window should pop up for you automatically when you type:\nrstudio --x11  You may also see this output in your terminal:\ndebug1: client_input_channel_open: ctype x11 rchan 3 win 65536 max 16384 debug1: client_request_x11: request from 127.0.0.1 46360 debug1: x11_connect_display: $DISPLAY is launchd debug1: channel 1: new [x11] debug1: confirm x11  Which is good. In your XQuartz application, you would see something like this:\nNote that if the window doesn\u0026rsquo;t pop up, you can just select the X [marks the spot] in the taskbar to find your instance of XQuartz or similar, where your RStudio window should be patiently waiting, assuming you didn\u0026rsquo;t get any errors.\n","date":1623247872,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623247872,"objectID":"79db6037c01a3cecc796f1421ef6c529","permalink":"https://alexanderlabwhoi.github.io/post/2021-03-17-xquartz/","publishdate":"2021-06-09T10:11:12-04:00","relpermalink":"/post/2021-03-17-xquartz/","section":"post","summary":"RStudio was once my favorite and most often-used IDE. And there are still a lot of reasons to use it, even if you\u0026rsquo;ve gotten used to something like Spyder or Jupyter. RStudio is a clean IDE that allows you to simultaneously view your filesystem, environment variables, R scripts, and console, and is all very specifically tailored to R, which is important in many cases (R is an amazing but finicky language).","tags":["conda","HPC","computing","R","RStudio"],"title":"`XQuartz` and remote integration for RStudio","type":"post"},{"authors":["Harriet Alexander"],"categories":["computation"],"content":" Over the last few years I have made a transition in my workflow from doing most of my data exploration and plotting on my local machine (a 2016 MacBook laptop) to doing most of my plotting on a remote HPC (check out this other blog post for more on that). This workflow is very convenient for many reasons (keeping all files in one location, more extensibility and power for my notebooks that my personal laptop can\u0026rsquo;t handle, minimizing file storage on my local machine, etc. etc.).\nHowever, I could not get any of my font specifications to work in matplotlib (or any dependent programs) in my notebooks that I was running via ssh tunnel. Period.\nI constantly got some variation of findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans..:\nThis happened both when I directly specified the font in my notebook:\nimport matplotlib.pyplot as plt plt.rcParams[\u0026quot;font.family\u0026quot;] = \u0026quot;sans-serif\u0026quot;  And also when I modified my matplotlibrc file (more on that later).\nI figured I was simply doing something wrong and eventually gave up on trying to fix it and just succumbed to my fate of manually changing fonts post-plotting in Adobe Illustrator. I did this for\u0026hellip; several years.\nAnd then yesterday, I had an epiphany. My remote HPC probably doesn\u0026rsquo;t have Microsoft fonts installed.\nAs Arianna said (slacked, really) when I asked for her comments on this blog post: \u0026ldquo;This is because unlike an OS that we’re used to working with that’s all set up out-of-the-box, HPC admins don’t really have a need to use anything but some kind of Courier New.\u0026rdquo;\nToo true.\nAnd so, I present to you\u0026hellip;\nHow to specify fonts in matplotlib when you are working on a Linux system: First off, I highly recommend using conda environments to manage research computational environments\u0026ndash; it can enhance reproducibility and avoid package conflicts (more here). For the purposes of this tutorial, I will assume that you have created a conda environment that has matplotlib installed. For the sake of demonstration my environment is called general_plotting.\nConveniently, there is a conda package mscorefonts that contains all the \u0026ldquo;core fonts\u0026rdquo; that are used for the Web. These fonts include: Andale Mono, Arial, Arial Black, Comic Sans MS, Courier New, Georgia, Impact, Times New Roman, Trebuchet MS, Verdana, and Webdings. While not totally comprehensive, it provides enough font variety for my purposes.\nAfter having activated whatever environment you are using, you can install the mscorefonts package.\nconda activate general_plotting conda install -c conda-forge mscorefonts  This will install and make those fonts available to you within your environment. We are almost done\u0026ndash; but a few more steps just to make it work.\nI found that I had to delete my cached matplotlib folder. If you have conda installed I believe that it should be located in your home directory; however, if you can\u0026rsquo;t find it there leave me a comment below.\nrm ~/.cache/matplotlib -rf  Now, any adjustments you want to make should be fairly straightforward!\nNow, you should be able to specify a font in your notebook preamble as you wish:\nimport matplotlib matplotlib.rcParams['font.family'] = \u0026quot;sans-serif\u0026quot; matplotlib.rcParams['font.sans-serif'] = \u0026quot;Comic Sans MS\u0026quot;  The above would make all plots in the notebook default to Comic Sans.\nYou should also be able to specify fonts directly on some figure portion (e.g. y-label):\nfig, ax = plt.plot(x,y) #some random variables ax.set_ylabel(\u0026quot;First variable\u0026quot;, fontname=\u0026quot;Arial\u0026quot;, fontsize=12)  Setting a default font I generally like all my plots to look relatively similar. My preferred font is Arial \u0026ndash; it is tidy and generally easy to read. If you want to specify a default font for all your plots without having to specify the rcParams in each notebook you can modify the matplotlibrc file for your environment. You can think of the matplotlibrc as similar to your .bashrc \u0026ndash; it controls some of the basic functionality and preferences for your matplotlib plotting. This file can be a bit buried (especially when using conda environments). One easy way to find your file is to open a notebook within whatever environment you are using and type the following:\nimport matplotlib print(matplotlib.matplotlib_fname())  This command will print the absolute path to your matplotlibrc.\nWithin this file, I edited the file such that I uncommented the following lines and made the following specifications:\nfont.family: sans-serif font.sans-serif: Arial  As a random aside while you have your matplotlibrc file open and handy, you should consider changing your pdf.fonttype to TrueType rather than the default Type 3. If you go to edit or tinker with a .pdf file sometime down the line, you might be upset to discover that all the fonts have been converted to vectors (or outlined shapes of the fonts that can\u0026rsquo;t be easily altered). You can set your default behavior to be exporting editable TrueType fonts by finding pdf.fonttype in matplotlibrc and uncommenting it and changing it as follows:\npdf.fonttype: 42  You can now change your fonts as you please and not be stuck with DejaVu Sans (a font that I really, really, REALLY dislike).\nHappy plotting!\n[Thanks to Arianna for commenting on this blog post for me :)]\n","date":1617132312,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617132312,"objectID":"4adfd490a5dd1f83680dfd639545e0d3","permalink":"https://alexanderlabwhoi.github.io/post/2021-03-missingfont/","publishdate":"2021-03-30T15:25:12-04:00","relpermalink":"/post/2021-03-missingfont/","section":"post","summary":"How to install MS core fonts with conda","tags":["python","matplotlib","fonts","data visualization"],"title":"Fixing my matplotlib font woes","type":"post"},{"authors":["Harriet Alexander"],"categories":["News"],"content":"Are you a recent (or not so recent) graduate with an interest in biological oceanography or marine biology? Do you have molecular biology experience in the lab? Are you curious to learn more about how molecular biology can be used to study environmental systems?\nI am excited to announce that Carolyn Tepolt and I are looking to hire a joint research assistant (i.e. lab tech) to join our labs at the Woods Hole Oceanographic Institution.\nHere is the position description from the official job posting:\n The Alexander and Tepolt labs at WHOI are seeking a motivated individual with prior laboratory experience to support genetic research in marine systems as a technician. The Alexander lab studies the biogeochemical functioning and physiological ecology of eukaryotic plankton, and the Tepolt lab studies population genetics and rapid adaptation in marine invertebrates. Time will be split between these two labs, and the technician will be a full and participating member of both lab groups. We will train the right candidate on specific lab protocols, but the position requires some prior experience and comfort with general molecular biology techniques (e.g. pipetting, PCR, etc.). This position is primarily lab-based and is ideal for a candidate who wishes to gain experience in the application of advanced genetic and molecular approaches to the study of marine organisms and ocean ecosystems.\n This position is particularly exciting as it provides the opportunity to work directly in two labs that employ different cutting-edge molecular approaches to study distinct systems.\nIn my lab, we are interested in trying to understand the ecological role protists (microbial eukaryotes) in the broader marine environment\u0026ndash; with a particular focus on phytoplankton. We leverage a variety of approaches including genomics and transcriptomics to study individual microbial isolates as well as meta-genomics and meta-transcriptomics to study mixed communities of microbial organisms. In this position you would be involved in the culturing of various single-celled plankton, the design and execution of culture experiments, and the testing and execution of various molecular techniques (e.g. Nanopore sequencing for protistan genomics).\nThe Tepolt lab is focused understanding adaptation in marine populations of metazoans (that are many orders of magnitude larger than the protists we work on in my lab). I encourage you to checkout her website and her post on the job opening to learn more about the awesome projects she has going on.\nBoth Carolyn and I are committed to building a diverse research group that is united by their interest in ocean life (of all sizes)! So, if this position sounds interesting to you, we encourage you learn more and apply through the official job WHOI advertisement HERE. We will begin reviewing applications March 5, 2021, so please submit your application by that date to ensure full consideration.\n","date":1612898712,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612898712,"objectID":"49a19139fc8cf63c73a833e466efe1c6","permalink":"https://alexanderlabwhoi.github.io/post/2021-02-hiring/","publishdate":"2021-02-09T15:25:12-04:00","relpermalink":"/post/2021-02-hiring/","section":"post","summary":"Are you a recent (or not so recent) graduate with an interest in biological oceanography or marine biology? Do you have molecular biology experience in the lab? Are you curious to learn more about how molecular biology can be used to study environmental systems?\nI am excited to announce that Carolyn Tepolt and I are looking to hire a joint research assistant (i.e. lab tech) to join our labs at the Woods Hole Oceanographic Institution.","tags":["hiring","Alexander Lab","molecular","genomics"],"title":"We are hiring!","type":"post"},{"authors":["*Arianna I Krinos §*","Sarah K Hu","Natalie R Cohen","**Harriet Alexander**"],"categories":null,"content":"","date":1609477200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609477200,"objectID":"7c6f127054eab5fe9eea31d1992cde2d","permalink":"https://alexanderlabwhoi.github.io/publication/krinos-2021/","publishdate":"2021-01-01T00:00:00-05:00","relpermalink":"/publication/krinos-2021/","section":"publication","summary":"","tags":["taxonomy","software","eukaryotes","metagenome","metatranscriptome","MAGs"],"title":"EUKulele: Taxonomic annotation of the unsung eukaryotic microbes","type":"publication"},{"authors":["Erin L. McParland","Michael D. Lee","Eric A. Webb","**Harriet Alexander**","Naomi M. Levine"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"d4a35e3b02732d382ab920ddb0a90fcb","permalink":"https://alexanderlabwhoi.github.io/publication/mc-parland-2021/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/mc-parland-2021/","section":"publication","summary":"Dimethylsulfoniopropionate (DMSP) is an important organic carbon and sulfur source in the surface ocean that fuels microbial activity and significantly impacts Earth's climate. After three decades of research, the cellular role(s) of DMSP and environmental drivers of production remain enigmatic. Recent work suggests that cellular DMSP concentrations and changes in these concentrations in response to environmental stressors define two major groups of DMSP producers= high DMSP producers that contain ≥50 mM intracellular DMSP and low DMSP producers that contain less than 50 mM. Here we show that two recently described DMSP synthesis genes (DSYB and TpMT2) may differentiate these two DMSP phenotypes. A survey of prokaryotic and eukaryotic isolates found a significant correlation between the presence of DSYB and TpMT2 genes and previous measurements of high and low DMSP concentrations, respectively. Phylogenetic analysis demonstrated that DSYB and TpMT2 form two distinct clades. DSYB and TpMT2 were also found to be globally abundant in in situ surface communities, and their taxonomic annotations were similar to those observed for isolates. The strong correlation of the DSYB and TpMT2 synthesis genes with high and low producer phenotypes establishes a foundation for direct quantification of DMSP producers, enabling significantly improved predictions of DMSP in situ This article is protected by copyright.","tags":["MMETSP","transcriptome","DMSP"],"title":"DMSP synthesis genes distinguish two types of DMSP producer phenotypes","type":"publication"},{"authors":null,"categories":[],"content":"","date":1600272272,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600272272,"objectID":"27a13675ac4378d0c6ddc28164f3eed2","permalink":"https://alexanderlabwhoi.github.io/tmp/johnson-2020/","publishdate":"2020-09-16T12:04:32-04:00","relpermalink":"/tmp/johnson-2020/","section":"tmp","summary":"","tags":[],"title":"Johnson 2020","type":"tmp"},{"authors":null,"categories":[],"content":"","date":1600271917,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600271917,"objectID":"5dfe099fd388e8a63d57481b6e302576","permalink":"https://alexanderlabwhoi.github.io/content/publication/caron-2016/","publishdate":"2020-09-16T11:58:37-04:00","relpermalink":"/content/publication/caron-2016/","section":"content","summary":"","tags":[],"title":"Caron 2016","type":"content"},{"authors":["Arianna Krinos"],"categories":["computation","data management"],"content":" There are a lot of reasons to set up Google Drive integration on your remote HPC system. In particular if your institution has provided you access to G Suite, there is a lot of storage available on Google Drive. Sometimes you might want to access files from multiple HPC systems, or have them at your fingertips on your local machine in addition to a remote server. This blog post will go over how to set up and use rclone with Google Drive on your remote server. You can learn more about rclone and how to use it here.\nTo get started, you\u0026rsquo;ll need to have a valid Google Drive account (personal or G Suite), and you\u0026rsquo;ll need to download rclone on your HPC server. The easiest way to install rclone is using Anaconda. You can do this by setting up a brand new conda environment, or by adding rclone to an existing environment. To create a brand new conda environment that you\u0026rsquo;ll always use each time you rclone, run something like the following command:\nconda create -n rcloneenv -c conda-forge rclone  And to just install rclone in an existing environment, type conda install -c conda-forge rclone. Be careful about potential dependencies!\nI had one HPC system I tried this on where the conda installation of rclone just didn\u0026rsquo;t want to work. In that case, I followed these steps:\ncurl -O https://downloads.rclone.org/rclone-current-linux-amd64.zip unzip rclone-current-linux-amd64.zip chmod 755 rclone-*-linux-amd64/rclone  Which downloads the base rclone package using curl, unpacks it, and then changes the rclone binary to an executable file, respectively. You might want to do this in a software downloads folder, and then execute:\nalias rclone='rclone-*-linux-amd64/rclone'  And/or add the directory to \\$PATH, using PATH=\\$PATH:rclone-*-linux-amd64/rclone', so that you can runrcloneanywhere. Note that this is a secondary, less desirable option to just usingconda`.\nNext, you\u0026rsquo;ll need to run rclone config.\nThe first few steps are easy. When prompted whether you\u0026rsquo;d like to add a new remote, set a configuration password, or quit, type \u0026ldquo;n\u0026rdquo;, for configuring a new remote. The next prompt will ask you for a name - this can be whatever you want. In this case, I\u0026rsquo;ll enter gdrive.\nThe next prompt asks what kind of remote this is, which should be set to \u0026ldquo;Google Drive\u0026rdquo;. It\u0026rsquo;s 13th on the list for me, so I enter 13. You can also just write \u0026ldquo;drive\u0026rdquo;.\nThe next step prompts you for a Google API Cloud ID. Getting one is not as complicated as it may sound! You\u0026rsquo;ll need to go to Google API Console (and make sure that you\u0026rsquo;re signed into the right Google account.\nFirst, go to Google API Console and make sure you\u0026rsquo;re logged in. It should look something like the below screen, at which point you select \u0026ldquo;New Project\u0026rdquo;.\nName the project whatever you like - it isn\u0026rsquo;t really important.\nNext, go to your project screen:\nYou\u0026rsquo;ll now need to navigate to the APIs Overview:\nIt\u0026rsquo;s tempting on this next screen to just click \u0026ldquo;Enable APIs and Services\u0026rdquo;, but that\u0026rsquo;s not what you want to do. Instead, head to the Credentials tab, then click \u0026ldquo;Configure Consent Screen\u0026rdquo;.\nThis splits off momentarily if you\u0026rsquo;re using a personal account vs. a G Suite account.\nCase 1: You\u0026rsquo;re using a \u0026ldquo;personal account\u0026rdquo; (no G Suite) You need to select \u0026ldquo;External\u0026rdquo; on the consent configuration screen.\nCase 2: You\u0026rsquo;re using a G Suite account You can use \u0026ldquo;External\u0026rdquo; (more inclusive), or \u0026ldquo;Internal\u0026rdquo;, if you want only users that are using an account that\u0026rsquo;s part of your G Suite organization to be able to modify/move these files (for example, if I am setting this up using an @whoi.edu email account, part of the WHOI G Suite Organization, using \u0026ldquo;Internal\u0026rdquo; will limit activity to other WHOI users). External will still require that users have a Google account, and that they are going through all the steps that you have.\nThe only option you need to change on the next screen is entering \u0026ldquo;rclone\u0026rdquo; for the application requesting access (though, ultimately, what you enter here is unlikely to make much of a difference.\nNext, you can scroll all the way down and hit \u0026ldquo;Save\u0026rdquo;, then navigate back to the Credentials tab, where you\u0026rsquo;ll now select that enticing \u0026ldquo;Create Credentials\u0026rdquo; button:\nAnd then selecting \u0026ldquo;OAuth client ID\u0026rdquo;. For \u0026ldquo;Application Type\u0026rdquo;, select \u0026ldquo;Desktop app.\u0026rdquo; What you name it doesn\u0026rsquo;t matter.\nAnd voilà! You now have a Client ID and a Client Secret, which you\u0026rsquo;ll copy sequentially and paste into the relevant prompts back at your remote server. The next question will ask what kind of consent you want to give Google Drive. If you\u0026rsquo;re interested in using rclone most effectively, and reading/writing/updating files, you\u0026rsquo;ll need to input \u0026ldquo;drive\u0026rdquo; or 1, in order to give Google Drive full file permissions.\nNow you can just hit enter a few times, to accept the default values for the following prompts:\n root_folder_id service_account_file Edit advanced config? (unless you\u0026rsquo;re interested in changing upload/download speeds/chunk sizes - advanced users only!)  But don\u0026rsquo;t use the default setting for the next prompt! It\u0026rsquo;ll be tempting to just keep clicking enter, but you need to type \u0026ldquo;n\u0026rdquo; if you\u0026rsquo;re on a remote server. Otherwise, the application will try to conduct authentication locally, which isn\u0026rsquo;t possible unless you\u0026rsquo;re using tunneling on your HPC. When you select \u0026ldquo;n\u0026rdquo;, rclone will generate a link for you to visit. Select the same Google account that you used for the previous steps, and then select \u0026ldquo;Allow.\u0026rdquo;\nThen, you\u0026rsquo;ll get another authentication code to copy/paste. You\u0026rsquo;ve probably never heard about Team Drives, so unless you have, select no for the next option (or hit enter). As long as everything looks okay, go ahead and accept the new remote, then enter \u0026ldquo;q\u0026rdquo; to exit the configuration.\nOne other important point is that you\u0026rsquo;ll need to be connected to the Internet when you are doing you rcloneing. This might be a little difficult if you are using a worker node on your HPC system that doesn\u0026rsquo;t have access to the Internet. Depending on your system\u0026rsquo;s configuration, you may need to use the login node when you use rclone (which should be avoided unless your system specifically limits Internet usage to the login node).\nNow your Google Drive is hooked up to your remote server!\nrclone copy /path/to/local/files gdrive:files  Will copy files from /path/to/local/files locally on your HPC to your Google Drive, in the files folder (make sure to use the name that you entered when you configured the remote).\nIf files change, either locally or on Google Drive, you can use sync to update them. For example, if I want to update Google Drive with my local files:\nrclone sync /path/to/local/files gdrive:files  But if I want to get the most recent Google Drive version:\nrclone sync gdrive:files /path/to/local/files  Only the second argument is modified/updated. Happy rcloneing!\n","date":1591125912,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591125912,"objectID":"667949bb16ffdb1c58e849362faf185e","permalink":"https://alexanderlabwhoi.github.io/post/2020-05-30-rclone/","publishdate":"2020-06-02T15:25:12-04:00","relpermalink":"/post/2020-05-30-rclone/","section":"post","summary":"There are a lot of reasons to set up Google Drive integration on your remote HPC system. In particular if your institution has provided you access to G Suite, there is a lot of storage available on Google Drive. Sometimes you might want to access files from multiple HPC systems, or have them at your fingertips on your local machine in addition to a remote server. This blog post will go over how to set up and use rclone with Google Drive on your remote server.","tags":["conda","HPC","computing","data storage"],"title":"Using `rclone` to manage data on a remote server via Google Drive","type":"post"},{"authors":["Winifred M Johnson","**Harriet Alexander**","Raven L Bier","Dan R Miller","Mario E Muscarella","Kathleen J Pitz","Heidi Smith"],"categories":null,"content":"","date":1590984000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590984000,"objectID":"9dc2ee2eb6e63df56d9aa05b322530e3","permalink":"https://alexanderlabwhoi.github.io/publication/johnson-2020/","publishdate":"2020-06-01T00:00:00-04:00","relpermalink":"/publication/johnson-2020/","section":"publication","summary":"Auxotrophy, or an organism's requirement for an exogenous source of an organic molecule, is widespread throughout species and ecosystems. Auxotrophy can result in obligate interactions between organisms, influencing ecosystem structure and community composition. We explore how auxotrophy-induced interactions between aquatic microorganisms affect microbial community structure and stability. While some studies have documented auxotrophy in aquatic microorganisms, these studies are not widespread, and we therefore do not know the full extent of auxotrophic interactions in aquatic environments. Current theoretical and experimental work suggests that auxotrophy links microbial community members through a complex web of metabolic dependencies. We discuss the proposed ways in which auxotrophy may enhance or undermine the stability of aquatic microbial communities, highlighting areas where our limited understanding of these interactions prevents us from being able to predict the ecological implications of auxotrophy. Finally, we examine an example of auxotrophy in harmful algal blooms to place this often theoretical discussion in a field context where auxotrophy may have implications for the development and robustness of algal bloom communities. We seek to draw attention to the relationship between auxotrophy and community stability in an effort to encourage further field and theoretical work that explores the underlying principles of microbial interactions.","tags":["modeling","auxtrophy","metabolites"],"title":"Auxotrophic interactions:  A stabilizing attribute of aquatic microbial communities?","type":"publication"},{"authors":["Harriet Alexander"],"categories":["conferences"],"content":" I am heading to Ocean Sciences Meeting next week in San Diego along with Laura Blum, an undergraduate researcher doing her thesis research with Maria Pachiadaki, Erin Eggleston, and myself. We have two presentations (that are bookending the meeting \u0026ndash; Monday and Friday) that are both related to work we have been doing looking at MAGs from the larger size fraction metagenomic datasets generated as part of the Tara Expedition.\nMicrobial drivers of nitrogen metabolism: Searching Tara Oceans metagenomes Laura Blum, Erin Eggleston, Maria Pachiadaki, \u0026amp; Harriet Alexander Poster# MM14A-0283 Monday 17 February 2020 16:00 - 18:00\nEukaryotic genome discovery: Scalable and automated retrieval of eukaryotic metagenome assembled genomes (MAGs) from a global-scale dataset Harriet Alexander \u0026amp; Sarah K Hu Session# OB53C: Toward BioGeoSCAPES: Exploring Molecular Drivers of Ocean Metabolism and Biogeochemistry Friday 21 February 2020 14:30 - 14:45, SDCC - 2, UL\n","date":1581357605,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581357605,"objectID":"c5bcbfb63913f193e3470d63649d5a82","permalink":"https://alexanderlabwhoi.github.io/post/2020-osm/","publishdate":"2020-02-10T13:00:05-05:00","relpermalink":"/post/2020-osm/","section":"post","summary":"I am heading to Ocean Sciences Meeting next week in San Diego along with Laura Blum, an undergraduate researcher doing her thesis research with Maria Pachiadaki, Erin Eggleston, and myself. We have two presentations (that are bookending the meeting \u0026ndash; Monday and Friday) that are both related to work we have been doing looking at MAGs from the larger size fraction metagenomic datasets generated as part of the Tara Expedition.","tags":["conferences"],"title":"Alexander Lab at OSM 2020","type":"post"},{"authors":["**Harriet Alexander**","Mónica Rouco","Sheean T Haley","Sonya T Dyhrman"],"categories":null,"content":"","date":1577854800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577854800,"objectID":"5f10eaf8fb6447fee213b7a8ff59140a","permalink":"https://alexanderlabwhoi.github.io/publication/alexander-2020/","publishdate":"2020-01-01T00:00:00-05:00","relpermalink":"/publication/alexander-2020/","section":"publication","summary":"The widespread coccolithophore *Emiliania huxleyi* is an abundant oceanic phytoplankton, impacting the global cycling of carbon through both photosynthesis and calcification. Here, we examined the transcriptional responses of populations of *E. huxleyi* in the North Pacific Subtropical Gyre to shifts in the nutrient environment. Using a metatranscriptomic approach, nutrient‐amended microcosm studies were used to track the global metabolism of *E. huxleyi*. The addition of nitrate led to significant changes in transcript abundance for gene pathways involved in nitrogen and phosphorus metabolism, with a decrease in the abundance of genes involved in the acquisition of nitrogen (e.g. N‐transporters) and an increase in the abundance of genes associated with phosphate acquisition (e.g. phosphatases). Simultaneously, after the addition of nitrate, genes associated with calcification and genes unique to the diploid life stages of *E. huxleyi* significantly increased. These results suggest that nitrogen is a major driver of the physiological ecology of *E. huxleyi* in this system and further suggest that the addition of nitrate drives shifts in the dominant life‐stage of the population. Together, these results underscore the importance of phenotypic plasticity to the success of *E. huxleyi*, a characteristic that likely underpins its ability to thrive across a variety of marine environments.","tags":["metatranscriptome","Ehux","nutrient physiology"],"title":"Transcriptional response of Emiliania huxleyi under changing nutrient environments in the North Pacific Subtropical Gyre","type":"publication"},{"authors":["Arianna Krinos"],"categories":["computation","bioinformatics","python"],"content":" Harriet wrote an awesome blog post here about how to use a Jupyter notebook connected to a remote server via SSH tunneling. Unfortunately, depending on your existing setup on your computer, you may have a few extra steps to wrangle if you have a PC. In this blog post, I\u0026rsquo;ll highlight what you should do after you get the Jupyter notebook running on the remote. In other words, you should have already gotten to the step where you spin up the Jupyter notebook, as in:\njupyter notebook --no-browser --port=8888  To get to this point, you will have probably used/installed PuTTY or a similar SSH client on your PC. This can be tricky as well if you\u0026rsquo;ve never done it before, so if you\u0026rsquo;re having trouble up to this point, I encourage you to check out a tutorial like this one.\nAfter you get the jupyter notebook spun up, you\u0026rsquo;re done working on the remote, and you have a few options for how to access some sort of terminal on your computer in order to tunnel the jupyter notebook to your local machine, so that you can use it in your browser like normal. You can set this up via port forwarding in PuTTY, but this limits the ports you can use for either the notebook on the remote or the port you\u0026rsquo;re forwarding to on your own computer. My favorite option is now to use Windows PowerShell, which is a native application on Windows computers (i.e., you get this pre-loaded on your machine). There are a few steps to this, but I think it\u0026rsquo;s ultimately the least amount of pain. I\u0026rsquo;ll also talk about two other options: MinTTY (I prefer to use it through CygWin) and the PuTTY port forwarding option.\nOption 1: Windows PowerShell Once you get the hang of it, I think of this as the simplest option. I mentioned that PowerShell is a native install on Windows, however, to get some of the newest features, you might have to upgrade to PowerShell 6, which can be done through PowerShell itself. Just open up PowerShell and type:\niex \u0026quot;\u0026amp; { $(irm https://aka.ms/install-powershell.ps1) } -UseMSI\u0026quot;  This will cause a normal Windows installer a la XP to spin up. The default settings are probably fine, but feel free to choose anything you think you need (don\u0026rsquo;t worry, there aren\u0026rsquo;t too many options). If you had the old version of PowerShell, you\u0026rsquo;ll now need to open a separate window to get to PowerShell 6. It\u0026rsquo;s a bit of a pain, but at least in PowerShell 6 you can paste into the terminal using Ctrl+V instead of left click! Silver linings.\nSide note: you probably (while you\u0026rsquo;re downloading things) want to take this opportunity to download an installer like scoop or chocolatey. I like scoop better, but chocolatey has a nice GUI. It\u0026rsquo;ll make your life easier in the future. You can install scoop, for example, with the following single line on PowerShell 6:\niex (new-object net.webclient).downloadstring('https://get.scoop.sh')  Now you\u0026rsquo;ve got a PowerShell\u0026hellip;but you\u0026rsquo;ll probably not be too happy when you try to ssh:\nThere\u0026rsquo;s another step here, but a pretty simple one if you use the power of git, which I assume here that you\u0026rsquo;ve already got installed on your machine. For most people, Git has ssh included and is already installed in the base executable directory. So instead of installing anything extra, we are just going to add where ssh is located via Git to our $PATH. Note that we could have done this without PowerShell 6, but note that as a Windows user you\u0026rsquo;re probably going to want PowerShell 6 and Chocolatey/Scoop regardless.\nSo to add the Git folder we need to the $PATH, we run the following two lines:\n$new_path = \u0026quot;$env:PATH;C:/Program Files/Git/usr/bin\u0026quot; [Environment]::setEnvironmentVariable(\u0026quot;path\u0026quot;, $new_path)  This is what I found worked best for me. If that in\u0026rsquo;t the location of Git for you (pretty unlikely on a Windows machine), you\u0026rsquo;ll need to go searching for it. The first and easiest line of defense is to go to your C drive and look for related \u0026ldquo;Program File\u0026rdquo; locations.\nNow, ssh should work, and you should be able to use the code that Harriet provided in [her post]() to get your Jupyter notebook tunneled. Try:\nssh -t -t \u0026lt;my username\u0026gt;@\u0026lt;some HPC address, such as hpc.mit.edu\u0026gt; -L 8888:localhost:8888 ssh \u0026lt;our node\u0026gt; -L 8888:localhost:8888  (Replace the items in brackets with your specific situation). If this gives you issues, a good fix is to put colons before each 8888 instance (after the space). This can help Windows understand that what you\u0026rsquo;re signaling is a port.\nYou should be good to go! Just type localhost:8888 in your web browser and see if that works. One other thing: if you get a password screen, this isn\u0026rsquo;t your HPC password or anything. This is a Jupyter password. You can either (if you haven\u0026rsquo;t set a password) insert the token that should be displayed (a bunch of letters and numbers) in the remote, or (if you don\u0026rsquo;t see this), you\u0026rsquo;ll need to set a password. Back out of your running Jupyter notebook on the remote, and run jupyter notebook password. This will prompt you to write and verify a password, which you\u0026rsquo;ll later enter into the window you see on your local machine, once you set up your SSH tunnel again. If this doesn\u0026rsquo;t work, it might be because the config file isn\u0026rsquo;t always created automatically. Run touch ~/.jupyter/jupyter_notebook_config.json to solve the issue (you might have to run mkdir ~/.jupyter first, depending on just how much isn\u0026rsquo;t set up).\nOption 2: MinTTY I like using CygWin for manipulating MinTTY (take it from their website: \u0026ldquo;get that Linux feeling\u0026hellip;on Windows\u0026rdquo; - don\u0026rsquo;t mind if I do!). Make sure that you select MinTTY and its associated packages and debuggers when setting up CygWin. This will save you the trouble of having to download MinTTY separately. You also need to make sure openssh and associated options are checked. If you forget anything, you can always re-run setup.exe from your Downloads or wherever you\u0026rsquo;ve placed it. After you go through the motions of selecting a mirror and everything, it\u0026rsquo;ll allow you to update the packages you\u0026rsquo;re using rather than starting completely from scratch.\nNow, at this point, you can either run ssh directly from the CygWin terminal (you may want to run as administrator just in case when you\u0026rsquo;re starting out), or through MinTTY. Note that you can also use CygWin in place of PuTTY or the like to connect to the remote server you\u0026rsquo;re using in the first place.\nRefer to the PowerShell section if you need help with setting up the tunneled Jupyter notebook beyond what is available in the base instructions.\nOption 3: PuTTY I won\u0026rsquo;t say much about this option, because it\u0026rsquo;s honestly the option I like the least. But if you really want to, you can set up port forwarding directly through your PuTTY session. This way, when you start up your Jupyter notebook on a port, say 8888 like we\u0026rsquo;ve been using, it will be automatically tunneled to your local machine (theoretically\u0026hellip;but I\u0026rsquo;d be criticized by PuTTY enthusiasts for adding that caveat!).\nFirst, open up PuTTY and make sure you\u0026rsquo;ve got all of your favorite settings loaded in, say by saving your profile.\nNext, go to Connection: SSH: Auth: Tunnels.\nNext, you need to change a few things. You need to tell PuTTY that your source port is on the remote, and then give the port on the remote as well as the localhost port you want to receive the connection:\nYou also need to check the two boxes up top to make sure the ports are listening to each other (I forgot this while I was constructing this post).\nIf this worked, you should be all set. Just make sure the ports match. I don\u0026rsquo;t have a lot of luck with this and in the past I\u0026rsquo;ve used Firefox port forwarding to get it to work. Chrome is my browser of choice, so for multiple reasons I prefer options 1 and 2 in this post!\n","date":1563909912,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563909912,"objectID":"305e90097255d7ac72fc1bc448295e0d","permalink":"https://alexanderlabwhoi.github.io/post/2019-07-24-slurm-win/","publishdate":"2019-07-23T15:25:12-04:00","relpermalink":"/post/2019-07-24-slurm-win/","section":"post","summary":"Harriet wrote an awesome blog post here about how to use a Jupyter notebook connected to a remote server via SSH tunneling. Unfortunately, depending on your existing setup on your computer, you may have a few extra steps to wrangle if you have a PC. In this blog post, I\u0026rsquo;ll highlight what you should do after you get the Jupyter notebook running on the remote. In other words, you should have already gotten to the step where you spin up the Jupyter notebook, as in:","tags":["conda","jupyter","HPC","computing","data analysis"],"title":"Running Jupyter Notebooks Remotely Using Windows","type":"post"},{"authors":["Sarah Hu"],"categories":[],"content":" I recently found myself setting up a new computer, spinning up ~23864283.34 new projects in R, and wanting to stay somewhat organized in the process. After doing some digging, I decided to give Anaconda environments a try to have the ability to run different versions of R and do this across R on the command line, RStudio, and jupyter notebooks (IRKernel). I have decided to share my process.\nInstalling Anaconda First, let\u0026rsquo;s make sure Anaconda is installed. If you do not have Anaconda, follow instructions here to install.\nOnce you have Anaconda installed, check the version.\nconda -V # I'm currently running v4.6.8 which conda # Location on my computer: /Users/shu/anaconda3  Why did you make me install anaconda? Briefly, Anaconda, or conda, is a software package manager. Anaconda will make your scientific programming life easier by facilitating software installations for many programs or packages. You can google \u0026lsquo;conda install for software X\u0026rsquo;, and often find a single line command to install the most recent version of that software.\nEven better, Anaconda allows you to create compartmentalized computational environments (called \u0026ldquo;conda environments\u0026rdquo; in this post) where you can install any mixture of things you require without accidentally messing up other software downloads on your computer. You can use these conda environments in any number of ways\u0026ndash; they can be program-specific (for the running of one specific tool) or project-specific (to store versions for a whole workflow). For instance, I\u0026rsquo;m working on a collaborative project which requires building bioinformatic pipelines with several different programs. It is key that my collaborator and I maintain the same versions of software, so code that we write individually can be easily shared. For this, we created and shared a conda environment. More info on getting started with conda here.\nConda environments have been widely adopted as a means of facilitating reproducible computation across the Python ecosystem\u0026ndash; but is now expanding to include other programming languages such as R (more here).\nBelow, documents how I have built up conda environments to run specific versions of R, including RStudio and jupyter notebooks.\nCreating a conda environment for R First, list the conda environments you have locally:\nconda info --envs  The output should list all the conda environments you have created. An asterisk will denote where you currently are, likely \u0026lsquo;base\u0026rsquo;. Additionally, your command line will be preceded with \u0026lsquo;(base)\u0026rsquo; to denote you are in your base conda environment.\nFirst, we will generate a specific R environment for R v3.5.1 (most recent version available through Anaconda, as of April 2019).\nconda create --name r_3.5.1 -c r r=3.5.1 r-essentials   What is happening in this command?\n \u0026lsquo;conda create \u0026ndash;name r_3.5.1\u0026rsquo; creates a new conda environment that will be called \u0026lsquo;r_3.5.1\u0026rsquo;. \u0026rsquo;-c r r=3.5.1\u0026rsquo; specifies to use the \u0026lsquo;r\u0026rsquo; channel for installation. In this particular case, we also specify r=3.5.1. Without this last part, conda will automatically install the most recent version of R. \u0026lsquo;r-essentials\u0026rsquo;, upon installing R-related packages, you need to add \u0026lsquo;r-\u0026rsquo; ahead of the package name. r-essentials is a package bundle which has a lot of the most basic and commonly used R packages. A full list of packages available for conda install.   This installation will take a few minutes and at the end you should see something like the following:\npip 19.1############################################################################################################################################## | 100% r-highr 0.7########################################################################################################################################### | 100% r-sourcetools 0.1.7################################################################################################################################### | 100% Preparing transaction: done Verifying transaction: done Executing transaction: done # # To activate this environment, use: # \u0026gt; source activate r_3.5.1 # # To deactivate an active environment, use: # \u0026gt; source deactivate #  Now, list your conda environments again with conda info --envs. You should now see the new environment listed:\nbase * /Users/shu/anaconda3 r_3.5.1 /Users/shu/anaconda3/envs/r_3.5.1  To activate your newly created R environment type:\nsource activate r_3.5.1  Your terminal prompt should now be preceded by your conda environment name, in this case (r_3.5.1), and the asterisk should be listed next to r_3.5.1 rather than basein the conda info --envs output.\nCheck that the correct R version is running We can check the version of R in two ways.\nFirst, launch R by typing R, you should see R version 3.5.1 (\u0026ldquo;Feather Spray\u0026rdquo;) load. IF you got an error about the \u0026ldquo;java\u0026rdquo; command-line tool, visit this site to fix it.. I got this error using anything \u0026gt;10.10 on Mac OS X.\nThe first line after typing R should be:\nR version 3.5.1 (2018-07-02) -- \u0026quot;Feather Spray\u0026quot; Copyright (C) 2018 The R Foundation for Statistical Computing Platform: x86_64-apple-darwin13.4.0 (64-bit)  Because we created this R environment and specified r-essentials, many popular packages have already been loaded. Check this by loading some basic libraries library(ggplot2) or library(dplyr). Convenient, eh?\nQuit R quit(), but don\u0026rsquo;t deactivate the r_3.5.1 environment.\nSecond, we can also directly check which R is being called by our $PATH in the conda environment by typing which R. This should output something like the following:\n/Users/shu/anaconda3/envs/r_3.5.1/bin/R  This indicates that R is being called from the bin in your newly created conda environment (anaconda3/envs/r_3.5.1/bin/).\nKeep this path in mind as we set up jupyter notebook and RStudio below, as they also need to launch R from this same location.\nCreating a jupyter notebook in R Jupyter notebook for R was included in the r-essential package download. To launch, type:\njupyter notebook  This will open the jupyter notebook home screen in browser window, showing your current directory. At the top right, you can click New and see which kernels you can use to start a notebook (hopefully, Python 3 or R).\nJupyter notebook dropdown\nSelect R under the \u0026lsquo;New\u0026rsquo; dropdown menu (you can also launch a Python3 notebook\u0026hellip; but that isn\u0026rsquo;t what we are doing right now), and a new Untitled.ipynb will open. In the first cell, enter version and execute the cell (SHIFT+ENTER to execute). This should print out the version of R that the notebook is running\u0026ndash; which should be 3.5.1. You can also ensure this R notebook is running properly by loading some R libraries.\nTo close the notebook, click Logout at the top right of the notebook open in your browser and/or CTRL+C in the terminal from where you launched the notebook. Close the browser windows.\nOne of the major issues I have encountered is Jupyter calling the incorrect path (more below).\nIf for some reason when you create this new notebook and you notice that the version is incorrect you can dig a bit deeper into the kernel details. To do this, you can\u0026rsquo;t use which R (as above) as that doesn\u0026rsquo;t answer where Jupyter is looking. Rather, you can use jupyter kernelspec, a command that I have found invaluable in troubleshooting any PATH issues in jupyter notebooks see twitter feed for that eureka moment.\nTo see what paths are being called by jupyter notebook type this:\njupyter kernelspec list --json  As an example, here is my output:\n{ \u0026quot;kernelspecs\u0026quot;: { \u0026quot;python3\u0026quot;: { \u0026quot;resource_dir\u0026quot;: \u0026quot;/Users/shu/anaconda3/envs/r_3.5.1/share/jupyter/kernels/python3\u0026quot;, \u0026quot;spec\u0026quot;: { \u0026quot;argv\u0026quot;: [ \u0026quot;/Users/shu/anaconda3/envs/r_3.5.1/bin/python\u0026quot;, \u0026quot;-m\u0026quot;, \u0026quot;ipykernel_launcher\u0026quot;, \u0026quot;-f\u0026quot;, \u0026quot;{connection_file}\u0026quot; ], \u0026quot;env\u0026quot;: {}, \u0026quot;display_name\u0026quot;: \u0026quot;Python 3\u0026quot;, \u0026quot;language\u0026quot;: \u0026quot;python\u0026quot;, \u0026quot;interrupt_mode\u0026quot;: \u0026quot;signal\u0026quot;, \u0026quot;metadata\u0026quot;: {} } }, \u0026quot;ir\u0026quot;: { \u0026quot;resource_dir\u0026quot;: \u0026quot;/Users/shu/anaconda3/envs/r_3.5.1/share/jupyter/kernels/ir\u0026quot;, \u0026quot;spec\u0026quot;: { \u0026quot;argv\u0026quot;: [ \u0026quot;R\u0026quot;, \u0026quot;--slave\u0026quot;, \u0026quot;-e\u0026quot;, \u0026quot;IRkernel::main()\u0026quot;, \u0026quot;--args\u0026quot;, \u0026quot;{connection_file}\u0026quot; ], \u0026quot;env\u0026quot;: {}, \u0026quot;display_name\u0026quot;: \u0026quot;R\u0026quot;, \u0026quot;language\u0026quot;: \u0026quot;R\u0026quot;, \u0026quot;interrupt_mode\u0026quot;: \u0026quot;signal\u0026quot;, \u0026quot;metadata\u0026quot;: {} } } } }  This command returns the paths (resource_dir) that the jupyter notebook when opened in the r_3.5.1 environment is accessing. The key point is that you should see that resource_dir for both python and ir (read: R) kernels should be within your conda environment path (.../anaconda3/envs/r_3.5.1/...).\nFrom my experience, a common issue I encountered was jupyter notebook trying to use an R version that was external to my environment (i.e. from an R version installed in my base environment or an old R version installed on my computer pre-anaconda). These issues manifest as IRKernel in jupyter notebook never becomes active or dying immediately after launch. This cause of this problem was revealed with the kernelspec command above.\nOne of the main lessons I learned in troubleshooting: don\u0026rsquo;t be afraid to destroy everything. More specifically, if you run into issues and find that one or all of these paths are not shuttling through the correct R conda environment of your choice\u0026hellip; destroy and rebuild. Use this command (jupyter kernelspec) to locate and destroy this other R option that jupyter notebook is defaulting to (but make sure you are not deleting another whole environment!).\nOpening RStudio within a conda environment To run RStudio in our environment, we can use the single-line conda command conda install -c r rstudio. However, it is important to specify the version of R that we want to install:\nconda install -c r r=3.5.1 rstudio # As of April 2019, this installs RStudio v1.1.456  Launch RStudio by typing rstudio. The first line in the RStudio console should be the same as when we launched R from the command line directly R version 3.5.1 (2018-07-02) -- \u0026quot;Feather Spray\u0026quot;. To close RStudio, you can close RStudio itself, or CTRL+C in the terminal from where you launched it.\nMoreover, by creating a specific alias, we can more precisely tell RStudio to open as a specific version (within a conda environment).\nFirst, perform a check to make sure RStudio is launching from the same location as R in this environment.\nwhich R # Output: /Users/sarahhu/anaconda3/envs/r_3.5.1/bin/R which rstudio # Output: /Users/sarahhu/anaconda3/envs/r_3.5.1/bin/rstudio  Copy the output from which R and open your bash profile (~/.bash_profile) using a text editor.\nAdd this line:\n# R version alias alias rstu3.5=\u0026quot;RSTUDIO_WHICH_R=/Users/shu/anaconda3/envs/r_3.5.1/bin/R open -a rstudio\u0026quot;  You can then source your bash profile to add that command to your current terminal session.\n# Source bash profile source ~/.bash_profile  Make sure you are still in your r_3.5.1 environment (sourcing your bash profile may place you back in your base environment). From the command line (in your r_3.5.1 environment) you can now type rstu3.5 to launch RStudio. I highly recommend opening RStudio from the command line using this alias you created! I found it was a much more stable way to work with RStudio.\nCreating a conda environment with a different R version Alright, so this is the moment of truth: can you successfully support two different R versions on your computer with conda environments?\nFirst, (if you aren\u0026rsquo;t already) logout of the r_3.5.1 conda environment with conda deactivate.\nLet\u0026rsquo;s try installing an older version of R\u0026ndash; 3.4.3. So, we will repeat the above instructions to create a new environment, but specify R\u0026ndash; 3.4.3.\n# Create new R environment with R version 3.4.3 conda create --name r_3.4.3 -c r r=3.4.3 r-essentials # Enter environment source activate r_3.4.3  Now, we can do the same checks that we did above. First, launch R on the command line and then spin up a jupyter notebook with an R kernel to check that they are both running R v3.4.3.\nNext, reinstall RStudio with R v.3.4.3\n# Install RStudio conda install -c r r=3.4.3 rstudio  And then ensure that you are running the correct version by opening RStudio from the command line (rstudio).\nYou should then double check the paths of R and rstudio within this environment:\n# Check outputs which R # Output: /Users/shu/anaconda3/envs/r_3.4.3/bin/R which rstudio # Output: /Users/shu/anaconda3/envs/r_3.4.3/bin/rstudio  If all things check out:\nCorrect R version in command line Correct R version in RStudio Correct R version in Jupyter Notebook Path of R aligns with your specified conda environment Path of rstudio aligns with your specified conda environment  You can safely go ahead and create a new alias within your .bash_profile :\nalias rstu3.4=\u0026quot;RSTUDIO_WHICH_R=/Users/shu/anaconda3/envs/r_3.4.3/bin/R open -a rstudio\u0026quot;  Finally, now when you list your conda environments conda info --envs you will see both new R environments.\n# conda environments: # base /Users/shu/anaconda3 r_3.4.3 * /Users/shu/anaconda3/envs/r_3.4.3 r_3.5.1 /Users/shu/anaconda3/envs/r_3.5.1  What other R versions are available for conda install? To check out other available R versions built through conda you can search the R channel using conda:\nconda search -c r r  Installing R packages To install other R packages, rather than use \u0026lsquo;install.packages()\u0026rsquo; in the R console, I recommend googling conda + PACKAGE-NAME or searching the conda R channel (conda search -f r-PACKAGENAME) for the package you want to install. When you are ready to install a package, make sure you are in the conda environment with whatever R version you want the package installed and type conda install -c r r-PACKAGENAME. This approach to R package installs is life changing, especially if you\u0026rsquo;re all too familiar with non-zero exit status or random failures when attempting to install R packages.\nYou can also view your list of installed R packages in your environment, conda list r-.\nConda channels for R package installation Some package groups require installation through a different conda channel. Two of the main ones I use are bioconductor Bioconda and conda-forge.\nTo set this up, configure your R environment by executing these commands in this exact order:\n# Type these commands in this exact order: conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge  To install a package using one of these channels, use \u0026ldquo;-c\u0026rdquo; in the conda install command. For instance to install compositions and cowplot, you need to use the conda-forge channel. See install information here.\nconda install -c conda-forge r-compositions conda install -c conda-forge r-cowplot  Other examples which require Bioconda:\nconda install -c bioconda bioconductor-decontam conda install -c bioconda bioconductor-deseq conda install -c bioconda bioconductor-edger  Final thoughts Don\u0026rsquo;t be afraid of destruction. Behind the scenes of this post, I completely removed and re-installed Anaconda on my computer at least twice and created and then deleted numerous environments. Starting with a clean slate turned out to be necessary.\nAnother reason to not shy away from destruction is that it is a way towards a more reproducible work process. After deleting and reinstalling Anaconda (and every other conda environment to my name), I was able to easily reinstate the environment my collaborator and I share with no issues (you can rebuild!). And now, I can return to whichever R version I need for a given project by navigating between my R conda environments. How much fun is it to be organized?.\nAnaconda was built with Python in mind. My motivation for this post was that it was difficult to find a working solution to partition R versions using conda environments (especially in conjunction with IRKernel and RStudio). Anaconda seems to work more succintly with python; as an example, I found that some older versions of R were not as stable as others. But R support is constantly growing! Also, check out the newly released RStudio 1.2, which will support Python\nThere\u0026rsquo;s more than one way to do this. If you have another solution or addition - let me know!\n","date":1556568390,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556568390,"objectID":"abb9d758f1fb8f8420cdaa33d055c814","permalink":"https://alexanderlabwhoi.github.io/post/anaconda-r-sarah/","publishdate":"2019-04-29T16:06:30-04:00","relpermalink":"/post/anaconda-r-sarah/","section":"post","summary":"I recently found myself setting up a new computer, spinning up ~23864283.34 new projects in R, and wanting to stay somewhat organized in the process. After doing some digging, I decided to give Anaconda environments a try to have the ability to run different versions of R and do this across R on the command line, RStudio, and jupyter notebooks (IRKernel). I have decided to share my process.\nInstalling Anaconda First, let\u0026rsquo;s make sure Anaconda is installed.","tags":["R","Anaconda","computation","best practices","RStudio","jupyter notebook"],"title":"Keeping my R life organized","type":"post"},{"authors":["Harriet Alexander"],"categories":["computation","bioinformatics","python"],"content":" I run the bulk of my bioinformatic analyses remotely on a server or HPC, as they require more computational power or space than I have on my local computer. Rather than transfer the intermediate byproducts of these analyses (which may often be very large) to my local machine, I prefer to examine and analyze the data remotely using Jupyter. As jupyter notebook are browser-based, if you run the command jupyter notebook on a remote machine you will not be able to automatically interact with the jupyter dashboard as you do not have access to a browser on the remote machine. Rather, you need to create a connection between your local browser and the remote Jupyter session. Here I am showing a special case, where you might want to run jupyter notebooks on a larger compute node via an interactive session with slurm.\nStarting your interactive job with slurm First things first: start up a tmux session (or screen if you prefer). If I am looking to have some program running for longer than I am wanting to keep a terminal window open \u0026ndash; tmux or screen are great options as they keep your session from timing out on remote machines. I have a preference for tmux, but it is really up to you.\ntmux new -s jupyter  Now that we are in our new tmux session, it is time to request an real-time run on the remote HPC. Using the slurm command srun, I am asking for 2 hours to run on two CPUs on a queue called main. You can customize this to your needs and resources by requesting more nodes, memory, etc.:\nsrun -p main --time=02:00:00 --ntasks-per-node 2 --pty bash  This will log you onto some node which will be noted in your command prompt. For example, my command line prompt changed from halexander@hpc to halexander@node1.\nI have many commands that I like in my .bash_profile that are not otherwise carried over to this new machine that we just logged into, so go ahead and source my bash profile (source ~/.bash_profile).\nCreating a conda environment and starting a jupyter notebook I like to run each of my various projects in its own conda environment. There are many reasons for this: reproducibility, control over program versions, dealing with conflicting package requirements, and, especially for on a shared compute resource (like an HPC), bypassing having root permission for installing programs. Another nice bonus for this particular case is that conda environments will automatically have jupyter installed. So, create a conda environment (if you haven\u0026rsquo;t already):\nconda create conda-env  and activate it:\nsource activate conda-env  You should now see that your terminal prompt has changed to something like the following, indicating that you are logged onto the interactive node and working within the conda-env environment:\n(conda-env) halexander@node1  One issue I encountered (that may be specific to my local HPC) that I want to note. If I try to run the command jupyter notebook right away I get the following error:\nTraceback (most recent call last): File \u0026quot;/address/home/halexander/.conda/envs/conda-env/lib/python3.6/site-packages/traitlets/traitlets.py\u0026quot;, line 528, in get value = obj._trait_values[self.name] KeyError: 'runtime_dir' .... PermissionError: [Errno 13] Permission denied: '/run/user/12746'  To get around this issue I found some help on StackOverflow. (Google is your friend for pesky errors like this.) This simple export command fixed my problem:\nexport XDG_RUNTIME_DIR=\u0026quot;\u0026quot;  Now, it is time to start up a jupyter notebook! On the remote machine type:\njupyter notebook --no-browser --port=8888  (Note, the default is for jupyter notebook to automatically open a browser \u0026ndash; but we can\u0026rsquo;t do that on a remote server, so we bypass that function with the --no-browser flag.)\nI regularly want to run this command and hate typing, so I went ahead and created a function to streamline this process:\nfunction jpt(){ jupyter notebook --no-browser --port=$1 }  This allows you to just type jpt and a port number and the command will be taken care of. If you want to use this function, simply copy the above and place it in your .bash_profile.\nNow, jpt 8888 will start a jupyter notebook on the port 8888.\nIf all is well, after running the above command, you should see something like the following:\n[I 14:22:55.931 NotebookApp] Writing notebook server cookie secret to /hpc/home/halexander/.local/share/jupyter/runtime/notebook_cookie_secret [I 14:23:01.371 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1 [I 14:23:01.371 NotebookApp] Serving notebooks from local directory: /vortexfs1/scratch/halexander/tara/sourmash-analysis [I 14:23:01.371 NotebookApp] The Jupyter Notebook is running at: [I 14:23:01.371 NotebookApp] http://localhost:8888/ [I 14:23:01.371 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).  ##Creating an SSH tunnel\nYou are now ready to create a tunnel from your local computer to the jupyter notebook running on the HPC.\nOpen a new terminal on your local machine. In the above example, I started a notebook on node1 at port 8888 and my username is halexander and the address of my HPC is hpc.address.edu. So, to create the tunnel I would type the following:\nssh -t -t halexander@hpc.address.edu -L 8888:localhost:8888 ssh node1 -L 8888:localhost:8888  This is a lot of typing, so it is simpler to create another bash function that you can put in your local .bash_profile.\nfunction jptnode(){ # Forwards port $1 from node $2 into port $1 on the local machine and listens to it ssh -t -t halexander@hpc.address.edu -L $1:localhost:$1 ssh $2 -L $1:localhost:$1 }  You will of course want to customize the above function to contain your username in place of halexander and the address of your hpc in place of hpc.address.edu.\nThe $ indicates the input variables in this function, with $1 being the port you specified for the jupyter notebook and $2 being the name of the node you are running the notebook on. So, to use this function you would type:\njptnode 8888 node1  Once you run this, you should notice that your terminal window has been logged on to your remote HPC and then logged onto the requested node.\nYou can now open your browser of choice and go to localhost:8888 and you should see the jupyter dashboard. You should be able to start working in jupyter notebooks, downloading data, or do any other things you want to do through the jupyter dashboard. Make sure to shutdown the jupyter notebook when you are done.\nA few important notes 1) Before you connect to a remote server with jupyter notebook make sure that you have configured jupyter with password information. You can do this by editing the jupyter-notebook_config.json which is usually located in ~/.jupyter or by typing:\njupyter notebook password  which will prompt you for a password that will be used for future notebooks.\n2) Make sure you shutdown your jupyter notebook when you are done. To do this, you can log back onto the tmux session you started earlier (tmux a -t jupyter) where the jupyter notebook is running and use ctrl-C should shutdown the jupyter notebook. If you run into issues with a port still being used, chances are that a notebook is still running somewhere.\n","date":1552343413,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552343413,"objectID":"60c3e8b5371957de9cec0a703b4e8acf","permalink":"https://alexanderlabwhoi.github.io/post/2019-03-08_jpn_slurm/","publishdate":"2019-03-11T18:30:13-04:00","relpermalink":"/post/2019-03-08_jpn_slurm/","section":"post","summary":"Using SSH to tunnel into jupyter notebooks deployed remotely on slurm","tags":["conda","jupyter","HPC","computing","data analysis"],"title":"Running Jupyter Notebooks Remotely with Slurm","type":"post"},{"authors":["**Harriet Alexander**","Lisa K Johnson","C Titus Brown"],"categories":null,"content":"","date":1543640400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543640400,"objectID":"15d4d641a299e5dc325f409e0b286416","permalink":"https://alexanderlabwhoi.github.io/publication/alexander2018/","publishdate":"2018-12-01T00:00:00-05:00","relpermalink":"/publication/alexander2018/","section":"publication","summary":"DNA sequencing technology has revolutionized the field of biology, shifting biology from a data-limited to data-rich state. Central to the interpretation of sequencing data are the computational tools and approaches that convert raw data into biologically meaningful information. Both the tools and the generation of data are actively evolving, yet the practice of re-analysis of previously generated data with new tools is not commonplace. Re-analysis of existing data provides an affordable means of generating new information and will likely become more routine within biology, yet necessitates a new set of considerations for best practices and resource development. Here, we discuss several practices that we believe to be broadly applicable when re-analyzing data, especially when done by small research groups.","tags":["reproducibility","data reuse","open data","automated pipeline","re-analysis"],"title":"Keeping it light: (Re)analyzing community-wide datasets without major infrastructure","type":"publication"},{"authors":["Lisa K Johnson","**Harriet Alexander**","C Titus Brown"],"categories":null,"content":"","date":1543640400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543640400,"objectID":"324d7980477401275112523c512e91a8","permalink":"https://alexanderlabwhoi.github.io/publication/johnson2018/","publishdate":"2018-12-01T00:00:00-05:00","relpermalink":"/publication/johnson2018/","section":"publication","summary":"**Background** De novo transcriptome assemblies are required prior to analyzing RNAseq data from a species without an existing reference genome or transcriptome. Despite the prevalence of transcriptomic studies, the effects of using different workflows, or “pipelines”, on the resulting assemblies are poorly understood. Here, a pipeline was programmatically automated and used to assemble and annotate raw transcriptomic short read data collected by the Marine Microbial Eukaryotic Transcriptome Sequencing Project (MMETSP). The resulting transcriptome assemblies were evaluated and compared against assemblies that were previously generated with a different pipeline developed by the National Center for Genome Research (NCGR). **Results** New transcriptome assemblies contained the majority of previous contigs as well as new content. On average, 7.8% of the annotated contigs in the new assemblies were novel gene names not found in the previous assemblies. Taxonomic trends were observed in the assembly metrics. Assemblies from the Dinoflagellata showed a higher number of contigs and unique k-mers than transcriptomes from other phyla while assemblies from Ciliophora had a lower percentage of open reading frames compared to other phyla. **Conclusions** Given current bioinformatics approaches, there is no single ‘best’ reference transcriptome for a particular set of raw data. As the optimum transcriptome is a moving target, improving (or not) with new tools and approaches, automated and programmable pipelines are invaluable for managing the computationally-intensive tasks required for re-processing large sets of samples with revised pipelines and ensuring a common evaluation workflow is applied to all samples. Thus, re-assembling existing data with new tools using automated and programmable pipelines may yield more accurate identification of taxon-specific trends across samples in addition to novel and useful products for the community.","tags":["de novo assembly","transcriptome","marine microbial eukaryote","protist","MMETSP","automated pipeline","re-analysis"],"title":"Re-assembly, quality evaluation, and annotation of 678 microbial eukaryotic reference transcriptomes","type":"publication"},{"authors":["Harriet Alexander"],"categories":[],"content":"Two months ago, I started in the Biology Department at the Woods Hole Oceanographic Institution as an Assistant Scientist. The processes has been otherworldly. Suddenly I find myself the steward of my own ship (read: lab). My emotional state seems to oscillate between excited exhilaration and existential terror (O 3 kHz). I have spent the last 11 (or more) years working towards this very moment\u0026ndash; so it is surreal to have officially arrived. And now\u0026hellip; what? Grants, papers, setting up a wet lab, navigating tenure, defining my scientific brand?\nOne thing I would like to practice more than I have during previous career stages is making writing more of a habit. Voicing myself in a public sphere has always made me nervous (reference that time in first grade when I got a nosebleed while leading the class in the pledge of allegiance). Be it giving a scientific talk, speaking up in a meeting or seminar, or tweeting (oh hey, you, follow me @nekton4plankton), I have to push by the jitters and psych myself up a bit.\nSo, consider this blog my attempt to habituate myself to making my thoughts more public. Topics to be covered will likely include: science, new PI life, computational blunders and wins, and life in a small, science-y fishing village.\n","date":1541975712,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541975712,"objectID":"8b160374fba5235910cf01d140c20932","permalink":"https://alexanderlabwhoi.github.io/post/first/","publishdate":"2018-11-11T18:35:12-04:00","relpermalink":"/post/first/","section":"post","summary":"Leveling up: welcome to your independent scientific position","tags":["new PI","lab setup"],"title":"Hello, world.","type":"post"},{"authors":["Evan Bolyen","Jai Ram Rideout","Matthew R Dillon","Nicholas A Bokulich","Christian Abnet","Gabriel A Al-Ghalith","**Harriet Alexander**","Eric J Alm","Manimozhiyan Arumugam","Francesco Asnicar","Yang Bai","Jordan E Bisanz","Kyle Bittinger","Asker Brejnrod","Colin J Brislawn","C Titus Brown","Benjamin J Callahan","Andrés Mauricio Caraballo-Rodríguez","John Chase","Emily Cope","Ricardo Da Silva","Pieter C Dorrestein","Gavin M Douglas","Daniel M Durall","Claire Duvallet","Christian F Edwardson","Madeleine Ernst","Mehrbod Estaki","Jennifer Fouquier","Julia M Gauglitz","Deanna L Gibson","Antonio Gonzalez","Kestrel Gorlick","Jiarong Guo","Benjamin Hillmann","Susan Holmes","Hannes Holste","Curtis Huttenhower","Gavin Huttley","Stefan Janssen","Alan K Jarmusch","Lingjing Jiang","Benjamin Kaehler","Kyo Bin Kang","Christopher R Keefe","Paul Keim","Scott T Kelley","Dan Knights","Irina Koester","Tomasz Kosciolek","Jorden Kreps","Morgan GI Langille","Joslynn Lee","Ruth Ley","Yong-Xin Liu","Erikka Loftfield","Catherine Lozupone","Massoud Maher","Clarisse Marotz","Bryan D Martin","Daniel McDonald","Lauren J McIver","Alexey V Melnik","Jessica L Metcalf","Sydney C Morgan","Jamie Morton","Ahmad Turan Naimey","Jose A Navas-Molina","Louis Felix Nothias","Stephanie B Orchanian","Talima Pearson","Samuel L Peoples","Daniel Petras","Mary Lai Preuss","Elmar Pruesse","Lasse Buur Rasmussen","Adam Rivers","II Michael S Robeson","Patrick Rosenthal","Nicola Segata","Michael Shaffer","Arron Shiffer","Rashmi Sinha","Se Jin Song","John R Spear","Austin D Swafford","Luke R Thompson","Pedro J Torres","Pauline Trinh","Anupriya Tripathi","Peter J Turnbaugh","Sabah Ul-Hasan","Justin JJ van der Hooft","Fernando Vargas","Yoshiki Vázquez-Baeza","Emily Vogtmann","Max von Hippel","William Walters","Yunhu Wan","Mingxun Wang","Jonathan Warren","Kyle C Weber","Chase HD Williamson","Amy D Willis","Zhenjiang Zech Xu","Jesse R Zaneveld","Yilong Zhang","Rob Knight","J Gregory Caporaso"],"categories":null,"content":" Abstract We present QIIME 2, an open-source microbiome data science platform accessible to users spanning the microbiome research ecosystem, from scientists and engineers to clinicians and policy makers. QIIME 2 provides new features that will drive the next generation of microbiome research. These include interactive spatial and temporal analysis and visualization tools, support for metabolomics and shotgun metagenomics analysis, and automated data provenance tracking to ensure reproducible, transparent microbiome data science.\n","date":1538366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538366400,"objectID":"b98637705fdc9bbad020af852ca3621f","permalink":"https://alexanderlabwhoi.github.io/publication/bolyen2018/","publishdate":"2018-10-01T00:00:00-04:00","relpermalink":"/publication/bolyen2018/","section":"publication","summary":"Abstract We present QIIME 2, an open-source microbiome data science platform accessible to users spanning the microbiome research ecosystem, from scientists and engineers to clinicians and policy makers. QIIME 2 provides new features that will drive the next generation of microbiome research. These include interactive spatial and temporal analysis and visualization tools, support for metabolomics and shotgun metagenomics analysis, and automated data provenance tracking to ensure reproducible, transparent microbiome data science.","tags":["bioinformatics","data science","microbial ecology","microbiome","reproducibility","software"],"title":"QIIME 2: Reproducible, interactive, scalable, and extensible microbiome data science","type":"publication"},{"authors":["Mónica Rouco","Kyle R. Frischkorn","Sheean T. Haley","**Harriet Alexander**","Sonya T. Dyhrman"],"categories":null,"content":"","date":1527825600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527825600,"objectID":"a8e05d109f9d3647f8955d3451e7538a","permalink":"https://alexanderlabwhoi.github.io/publication/rouco2018/","publishdate":"2018-06-01T00:00:00-04:00","relpermalink":"/publication/rouco2018/","section":"publication","summary":"The N2-fixing cyanobacterium *Trichodesmium* is intensely studied because of the control this organism exerts over the cycling of carbon and nitrogen in the low nutrient ocean gyres. Although iron (Fe) and phosphorus (P) bioavailability are thought to be major drivers of *Trichodesmium* distributions and activities, identifying resource controls on *Trichodesmium* is challenging, as Fe and P are often organically complexed and their bioavailability to a single species in a mixed community is difficult to constrain. Further, Fe and P geochemistries are linked through the activities of metalloenzymes, such as the alkaline phosphatases (APs) PhoX and PhoA, which are used by microbes to access dissolved organic P (DOP). Here we identified significant correlations between *Trichodesmium*-specific transcriptional patterns in the North Atlantic (NASG) and North Pacific Subtropical Gyres (NPSG) and patterns in Fe and P biogeochemistry, with the relative enrichment of Fe stress markers in the NPSG, and P stress markers in the NASG. We also observed the differential enrichment of Fe-requiring PhoX transcripts in the NASG and Fe-insensitive PhoA transcripts in the NPSG, suggesting that metalloenzyme switching may be used to mitigate Fe limitation of DOP metabolism in *Trichodesmium*. This trait may underpin *Trichodesmium* success across disparate ecosystems.","tags":["Trichodesmium","biogeochemistry","Station ALOHA","metatranscriptomics","qRT-PCR","phosphorus","metalloenzyme","gene expression"],"title":"Transcriptional patterns identify resource controls on the diazotroph Trichodesmium in the Atlantic and Pacific oceans","type":"publication"},{"authors":["Sarah K Hu","Zhenfeng Liu","**Harriet Alexander**","Victoria Campbell","Paige E Connell","Sonya T Dyhrman","Karla B Heidelberg","David A Caron"],"categories":null,"content":"","date":1522555200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522555200,"objectID":"908d464b0d55b1e1564582e7a1289b23","permalink":"https://alexanderlabwhoi.github.io/publication/hu2018/","publishdate":"2018-04-01T00:00:00-04:00","relpermalink":"/publication/hu2018/","section":"publication","summary":"A metatranscriptome study targeting the protistan community was conducted off the coast of Southern California, at the San Pedro Ocean Time‐series station at the surface, 150 m (oxycline), and 890 m to link putative metabolic patterns to distinct protistan lineages. Comparison of relative transcript abundances revealed depth‐related shifts in the nutritional modes of key taxonomic groups. Eukaryotic gene expression in the sunlit surface environment was dominated by phototrophs, such as diatoms and chlorophytes, and high abundances of transcripts associated with synthesis pathways (e.g., photosynthesis, carbon fixation, fatty acid synthesis). Sub‐euphotic depths (150 and 890 m) exhibited strong contributions from dinoflagellates and ciliates, and were characterized by transcripts relating to digestion or intracellular nutrient recycling (e.g., breakdown of fatty acids and V‐type ATPases). These transcriptional patterns underlie the distinct nutritional modes of ecologically important protistan lineages that drive marine food webs, and provide a framework to investigate trophic dynamics across diverse protistan communities.","tags":["de novo assembly","gene expression","metatranscriptomics","sub-euphotic","phytoplankton","Station ALOHA"],"title":"Shifting metabolic priorities among key protistan taxa within and below the euphotic zone","type":"publication"},{"authors":["Tamar Guy-Haim","**Harriet Alexander**","Tom W. Bell","Raven L. Bier","Lauren E. Bortolotti","Christian Briseño-Avena","Xiaoli Dong","Alison M. Flanagan","Julia Grosse","Lars Grossmann","Sarah Hasnain","Rachel Hovel","Cora A. Johnston","Dan R. Miller","Mario Muscarella","Akana E. Noto","Alexander J. Reisinger","Heidi J. Smith","Karen Stamieszkin"],"categories":null,"content":"","date":1512104400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512104400,"objectID":"d1cf6e7ebd539253a21c13cf4290876f","permalink":"https://alexanderlabwhoi.github.io/publication/guy-haim2017/","publishdate":"2017-12-01T00:00:00-05:00","relpermalink":"/publication/guy-haim2017/","section":"publication","summary":" **Background:** Mesocosm experiments have become increasingly popular in climate change research as they bridge the gap between small-scale, less realistic, microcosm experiments, and large-scale, more complex, natural systems. Characteristics of aquatic mesocosm designs (e.g., mesocosm volume, study duration, and replication) vary widely, potentially affecting the magnitude and direction of effect sizes measured in experiments. In this global systematic review we aim to identify the type, direction and strength of climate warming effects on aquatic species, communities and ecosystems in mesocosm experiments. Furthermore, we will investigate the context-dependency of the observed effects on several a priori determined effect moderators (ecological and methodological). Our conclusions will provide recommendations for aquatic scientists designing mesocosm experiments, as well as guidelines for interpretation of experimental results by scientists, policy-makers and the general public. **Methods:** We will conduct a systematic search using multiple online databases to gather evidence from the scientific literature on the effects of warming experimentally tested in aquatic mesocosms. Data from relevant studies will be extracted and used in a random effects meta-analysis to estimate the overall effect sizes of warming experiments on species performance, biodiversity and ecosystem functions. Experimental characteristics (e.g., mesocosm size and shape, replication-level, experimental duration and design, biogeographic region, community type, crossed manipulation) will be further analysed using subgroup analyses.","tags":["climate change","global warming","mesocosm","microcosm","aquatic","marine","estuarine","freshwater","experimental design","methodology"],"title":"What are the type, direction, and strength of species, community, and ecosystem responses to warming in aquatic mesocosm studies and their dependency on experimental characteristics? A systematic review protocol","type":"publication"},{"authors":["Sheean T. Haley","**Harriet Alexander**","Andrew R. Juhl","Sonya T. Dyhrman"],"categories":null,"content":"","date":1504238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504238400,"objectID":"cf1db8a6593d39e447e8f245d53d0420","permalink":"https://alexanderlabwhoi.github.io/publication/haley2017/","publishdate":"2017-09-01T00:00:00-04:00","relpermalink":"/publication/haley2017/","section":"publication","summary":"The marine eukaryotic alga *Heterosigma akashiwo* (Raphidophyceae) is known for forming ichthyotoxic harmful algal blooms (HABs). In the past 50 years, *H. akashiwo* blooms have increased, occurring globally in highly eutrophic coastal and estuarine systems. These systems often incur dramatic physicochemical changes, including macronutrient (nitrogen and phosphorus) enrichment and depletion, on short timescales. Here, *H. akashiwo* cultures grown under nutrient replete, low N and low P growth conditions were examined for changes in biochemical and physiological characteristics in concert with transcriptome sequencing to provide a mechanistic perspective on the metabolic processes involved in responding to N and P stress. There was a marked difference in the overall transcriptional pattern between low N and low P transcriptomes. Both nutrient stresses led to significant changes in the abundance of thousands of contigs related to a wide diversity of metabolic pathways, with limited overlap between the transcriptomic responses to low N and low P. Enriched contigs under low N included many related to nitrogen metabolism, acquisition, and transport. In addition, metabolic modules like photosynthesis and carbohydrate metabolism changed significantly under low N, coincident with treatment-specific changes in photosynthetic efficiency and particulate carbohydrate content. P-specific contigs responsible for P transport and organic P use were more enriched in the low P treatment than in the replete control and low N treatment. These results provide new insight into the genetic mechanisms that distinguish how this HAB species responds to these two common nutrient stresses, and the results can inform future field studies, linking transcriptional patterns to the physiological ecology of *H. akashiwo* in situ.","tags":["de novo assembly","gene expression","harmful algal bloom","Heterosigma akashiwo","phytoplankton","transcriptomics","nutrient","phosphorus"],"title":"Transcriptional response of the harmful raphidophyte Heterosigma akashiwo to nitrate and phosphate stress","type":"publication"},{"authors":["Matthew J. Harke","Andrew R. Juhl","Sheean T. Haley","**Harriet Alexander**","Sonya T. Dyhrman"],"categories":null,"content":"","date":1498881600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498881600,"objectID":"49e137936c394c54140b5a53d0f44cfa","permalink":"https://alexanderlabwhoi.github.io/publication/harke2017/","publishdate":"2017-07-01T00:00:00-04:00","relpermalink":"/publication/harke2017/","section":"publication","summary":"The concentration and composition of bioavailable nitrogen (N) and phosphorus (P) in the upper ocean shape eukaryotic phytoplankton communities and influence their physiological responses. Phytoplankton are known to exhibit similar physiological responses to limiting N and P conditions such as decreased growth rates, chlorosis, and increased assimilation of N and P. Are these responses similar at the molecular level across multiple species? To interrogate this question, five species from biogeochemically important, bloom-forming taxa (Bacillariophyta, Dinophyta, and Haptophyta) were grown under similar low N, low P, and replete nutrient conditions to identify transcriptional patterns and associated changes in biochemical pools related to N and P stress. Metabolic profiles, revealed through the transcriptomes of these taxa, clustered together based on species rather than nutrient stressor, suggesting that the global metabolic response to nutrient stresses was largely, but not exclusively, species-specific. Nutrient stress led to few transcriptional changes in the two dinoflagellates, consistent with other research. An orthologous group analysis examined functionally conserved (i.e., similarly changed) responses to nutrient stress and therefore focused on the diatom and haptophytes. Most conserved ortholog changes were specific to a single nutrient treatment, but a small number of orthologs were similarly changed under both N and P stress in 2 or more species. Many of these orthologs were related to photosynthesis and may represent generalized stress responses. A greater number of orthologs were conserved across more than one species under low P compared to low N. Screening the conserved orthologs for functions related to N and P metabolism revealed increased relative abundance of orthologs for nitrate, nitrite, ammonium, and amino acid transporters under N stress, and increased relative abundance of orthologs related to acquisition of inorganic and organic P substrates under P stress. Although the global transcriptional responses were dominated by species-specific changes, the analysis of conserved responses revealed functional similarities in resource acquisition pathways among different phytoplankton taxa. This overlap in nutrient stress responses observed among species may be useful for tracking the physiological ecology of phytoplankton field populations.","tags":["conserved response","MMETSP","nitrogen","phosphorus","phytoplankton","transcriptomics","diatom","gene expression"],"title":"Conserved transcriptional responses to nutrient stress in bloom-forming algae","type":"publication"},{"authors":["Mohammad Moniruzzaman","Louie L. Wurch","**Harriet Alexander**","Sonya T. Dyhrman","Christopher J. Gobler","Steven W. Wilhelm"],"categories":null,"content":"","date":1496289600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496289600,"objectID":"26800689e8c05cb30ea038f4a0db85ec","permalink":"https://alexanderlabwhoi.github.io/publication/moniruzzaman2017/","publishdate":"2017-06-01T00:00:00-04:00","relpermalink":"/publication/moniruzzaman2017/","section":"publication","summary":"Establishing virus–host relationships has historically relied on culture-dependent approaches. Here we report on the use of marine metatranscriptomics to probe virus–host relationships. Statistical co-occurrence analyses of dsDNA, ssRNA and dsRNA viral markers of polyadenylation-selected RNA sequences from microbial communities dominated by *Aureococcus anophagefferens* (Quantuck Bay, NY), and diatoms (Narragansett Bay, RI) show active infections by diverse giant viruses (NCLDVs) associated with algal and nonalgal hosts. Ongoing infections of *A. anophagefferens* by a known Mimiviridae (AaV) occur during bloom peak and decline. Bloom decline is also accompanied by increased activity of viruses other than AaV, including (+) ssRNA viruses. In Narragansett Bay, increased temporal resolution reveals active NCLDVs with both ‘boom-and-bust’ and ‘steady-state infection’-like ecologies that include known as well as novel virus–host interactions. Our approach offers a method for screening active viral infections and develops links between viruses and their potential hosts in situ. Our observations further demonstrate that previously unknown virus–host relationships in marine systems are abundant.","tags":["virus","metatranscriptomics","virus-host","phytoplankton","Aureococcus anophagefferens"],"title":"Virus-host relationships of marine single-celled eukaryotes resolved from metatranscriptomics","type":"publication"},{"authors":["Jennifer M. Durden","Jessica Y. Luo","**Harriet Alexander**","Alison M. Flanagan","Lars Grossmann"],"categories":null,"content":"","date":1483246800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483246800,"objectID":"e05e2c5b48eee76fd019833f51d78c77","permalink":"https://alexanderlabwhoi.github.io/publication/durden2017/","publishdate":"2017-01-01T00:00:00-05:00","relpermalink":"/publication/durden2017/","section":"publication","summary":"Got “Big Data“? Not sure how best to use it? Big Data is becoming an important facet of aquatic ecology, and researchers must learn to harness it to reap the rewards of using it. The benefits of using Big Data are many, and include advancements in scientific understanding at larger scales and higher resolution, applications to improving environmental management and policy, and public engagement. We aim to demystify the use of Big Data for individual scientists, and provide some food for thought for the aquatic ecology community on how to develop this sphere. To achieve this, we highlight six key challenges: (1) how to recognize if you have Big Data, (2) handling Big Data, (3) issues with classical analytical techniques, (4) verification of Big Data, (5) considerations for data sharing, and (6) community development of knowledge infrastructures. We then present approaches and tools which have been successfully applied to these challenges in aquatic ecology and other scientific fields.","tags":["big data","best practices","data sharing","cyber infrastructure"],"title":"Integrating 'big data' into aquatic ecology: Challenges and opportunities","type":"publication"},{"authors":["Elizabeth B Kujawinski","Krista Longnecker","**Harriet Alexander**","Sonya T Dyhrman","Cara L Fiore","Sheean T Haley","Winifred M Johnson"],"categories":null,"content":"","date":1483246800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483246800,"objectID":"b9b499357362a86e4a170516d297869c","permalink":"https://alexanderlabwhoi.github.io/publication/kujawinski2017/","publishdate":"2017-01-01T00:00:00-05:00","relpermalink":"/publication/kujawinski2017/","section":"publication","summary":"Marine eukaryotic phytoplankton adapt to low phosphorus (P) in the oceans through a variety of step‐wise mechanisms including lipid substitution and decreased nucleic acid content. Here, we examined the impact of low P concentrations on intracellular metabolites whose abundances can be quickly adjusted by cellular regulation within laboratory cultures of three model phytoplankton and in field samples from the Atlantic and Pacific Oceans. We quantified the relative abundances of monophosphate nucleotides and their corresponding nucleosides, using a combination of targeted and untargeted metabolomics methods. Under P‐deficient conditions, we observed a marked decrease in adenosine 5′‐monophosphate (AMP) with a concomitant increase in adenosine. This shift occurred within all detected pairs of monophosphate nucleotides and nucleosides, and was consistent with previous work showing transcriptional changes in nucleotide synthesis and salvage under P‐deficient conditions for model eukaryotes. In the field, we observed AMP‐to‐adenosine ratios that were similar to those in laboratory culture under P‐deficient conditions.","tags":["phosphorus","diatom","phytoplankton","metabolomics","nucelosides","transcriptomics","AMP"],"title":"Phosphorus availability regulates intracellular nucleotides in marine eukaryotic phytoplankton","type":"publication"},{"authors":["David A. Caron","**Harriet Alexander**","Andrew E. Allen","John M. Archibald","E. Virginia Armbrust","Charles Bachy","Callum J. Bell","Arvind Bharti","Sonya T. Dyhrman","Stephanie M. Guida","Karla B. Heidelberg","Jonathan Z. Kaye","Julia Metzner","Sarah R. Smith","Alexandra Z. Worden"],"categories":null,"content":"","date":1477972800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477972800,"objectID":"5748e147c7523545d20ce4479f9c4506","permalink":"https://alexanderlabwhoi.github.io/publication/caron2016/","publishdate":"2016-11-01T00:00:00-04:00","relpermalink":"/publication/caron2016/","section":"publication","summary":"Protists, which are single-celled eukaryotes, critically influence the ecology and chemistry of marine ecosystems, but genome-based studies of these organisms have lagged behind those of other microorganisms. However, recent transcriptomic studies of cultured species, complemented by meta-omics analyses of natural communities, have increased the amount of genetic information available for poorly represented branches on the tree of eukaryotic life. This information is providing insights into the adaptations and interactions between protists and other microorganisms and macroorganisms, but many of the genes sequenced show no similarity to sequences currently available in public databases. A better understanding of these newly discovered genes will lead to a deeper appreciation of the functional diversity and metabolic processes in the ocean. In this Review, we summarize recent developments in our understanding of the ecology, physiology and evolution of protists, derived from transcriptomic studies of cultured strains and natural communities, and discuss how these novel large-scale genetic datasets will be used in the future.","tags":["transcriptomics","MMETSP","protists","evolution","functional diversity"],"title":"Probing the evolution, ecology and physiology of marine protists using transcriptomics","type":"publication"},{"authors":["Mónica Rouco","Sheean T. Haley","**Harriet Alexander**","Samuel T. Wilson","David M. Karl","Sonya T. Dyhrman"],"categories":null,"content":" Abstract ","date":1475294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475294400,"objectID":"dc9adba77017a35eaf462bb84e3e319e","permalink":"https://alexanderlabwhoi.github.io/publication/rouco2016/","publishdate":"2016-10-01T00:00:00-04:00","relpermalink":"/publication/rouco2016/","section":"publication","summary":"Populations of nitrogen‐fixing cyanobacteria in the genus *Trichodesmium* are critical to ocean ecosystems, yet predicting patterns of *Trichodesmium* distribution and their role in ocean biogeochemistry is an ongoing challenge. This may, in part, be due to differences in the physiological ecology of *Trichodesmium* species, which are not typically considered independently in field studies. In this study, the abundance of the two dominant *Trichodesmium* clades (Clade I and Clade III) was investigated during a survey at Station ALOHA in the North Pacific Subtropical Gyre (NPSG) using a clade‐specific qPCR approach. While Clade I dominated the *Trichodesmium* community, Clade III abundance was 50% in some NPSG samples, in contrast to the western North Atlantic where Clade III abundance was always 80 m, while Clade III populations were only observed in the mixed layer and found to be significantly correlated with depth and temperature. These data suggest active niche partitioning of *Trichodesmium* species from different clades, as has been observed in other cyanobacteria. Tracking the distribution and physiology of *Trichodesmium* spp. would contribute to better predictions of the physiological ecology of this biogeochemically important genus in the present and future ocean.","tags":["Trichodesmium","biogeochemistry","Station ALOHA","diversity","qRT-PCR"],"title":"Variable depth distribution of Trichodesmium clades in the North Pacific Ocean","type":"publication"},{"authors":["**Harriet Alexander**","Mónica Rouco","Sheean T. Haley","Samuel T. Wilson","David M. Karl","Sonya T. Dyhrman"],"categories":null,"content":"","date":1446350400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446350400,"objectID":"8dced12528eb562e9d9b7d10cd42e23f","permalink":"https://alexanderlabwhoi.github.io/publication/alexander2015a/","publishdate":"2015-11-01T00:00:00-04:00","relpermalink":"/publication/alexander2015a/","section":"publication","summary":"A diverse microbial assemblage in the ocean is responsible for nearly half of global primary production. It has been hypothesized and experimentally demonstrated that nutrient loading can stimulate blooms of large eukaryotic phytoplankton in oligotrophic systems. Although central to balancing biogeochemical models, knowledge of the metabolic traits that govern the dynamics of these bloom-forming phytoplankton is limited. We used eukaryotic metatranscriptomic techniques to identify the metabolic basis of functional group-specific traits that may drive the shift between net heterotrophy and autotrophy in the oligotrophic ocean. Replicated blooms were simulated by deep seawater (DSW) addition to mimic nutrient loading in the North Pacific Subtropical Gyre, and the transcriptional responses of phytoplankton functional groups were assayed. Responses of the diatom, haptophyte, and dinoflagellate functional groups in simulated blooms were unique, with diatoms and haptophytes significantly (95% confidence) shifting their quantitative metabolic fingerprint from the in situ condition, whereas dinoflagellates showed little response. Significantly differentially abundant genes identified the importance of colimitation by nutrients, metals, and vitamins in eukaryotic phytoplankton metabolism and bloom formation in this system. The variable transcript allocation ratio, used to quantify transcript reallocation following DSW amendment, differed for diatoms and haptophytes, reflecting the long-standing paradigm of phytoplankton r- and K-type growth strategies. Although the underlying metabolic potential of the large eukaryotic phytoplankton was consistently present, the lack of a bloom during the study period suggests a crucial dependence on physical and biogeochemical forcing, which are susceptible to alteration with changing climate.","tags":["metatranscriptomics","phytoplankton","ecological traits","biogeochemistry","blooms","Station ALOHA","gene expression"],"title":"Functional group-specific traits drive phytoplankton dynamics in the oligotrophic ocean","type":"publication"},{"authors":["**Harriet Alexander**","Bethany D. Jenkins","Tatiana A. Rynearson","Sonya T. Dyhrman"],"categories":null,"content":"","date":1427860800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1427860800,"objectID":"8d3b5de4f175eabb73a6fe140301f07a","permalink":"https://alexanderlabwhoi.github.io/publication/alexander2015/","publishdate":"2015-04-01T00:00:00-04:00","relpermalink":"/publication/alexander2015/","section":"publication","summary":"Diverse communities of marine phytoplankton carry out half of global primary production. The vast diversity of the phytoplankton has long perplexed ecologists because these organisms coexist in an isotropic environment while competing for the same basic resources (e.g., inorganic nutrients). Differential niche partitioning of resources is one hypothesis to explain this “paradox of the plankton,” but it is difficult to quantify and track variation in phytoplankton metabolism in situ. Here, we use quantitative metatranscriptome analyses to examine pathways of nitrogen (N) and phosphorus (P) metabolism in diatoms that cooccur regularly in an estuary on the east coast of the United States (Narragansett Bay). Expression of known N and P metabolic pathways varied between diatoms, indicating apparent differences in resource utilization capacity that may prevent direct competition. Nutrient amendment incubations skewed N/P ratios, elucidating nutrient-responsive patterns of expression and facilitating a quantitative comparison between diatoms. The resource-responsive (RR) gene sets deviated in composition from the metabolic profile of the organism, being enriched in genes associated with N and P metabolism. Expression of the RR gene set varied over time and differed significantly between diatoms, resulting in opposite transcriptional responses to the same environment. Apparent differences in metabolic capacity and the expression of that capacity in the environment suggest that diatom-specific resource partitioning was occurring in Narragansett Bay. This high-resolution approach highlights the molecular underpinnings of diatom resource utilization and how cooccurring diatoms adjust their cellular physiology to partition their niche space.","tags":["phytoplankton","diatom","nutrient physiology","niche partitioning","metatranscriptomics","gene expression"],"title":"Metatranscriptome analyses indicate resource partitioning between diatoms in the field","type":"publication"},{"authors":["Alexis Fischer","Emily Moberg","**Harriet Alexander**","Emily Brownlee","Kristen Hunter-Cevera","Kathleen Pitz","Sarah Rosengard","Heidi Sosik"],"categories":null,"content":"","date":1393650000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1393650000,"objectID":"a1212c663ffb68cf6401b27b06996320","permalink":"https://alexanderlabwhoi.github.io/publication/fischer2014/","publishdate":"2014-03-01T00:00:00-05:00","relpermalink":"/publication/fischer2014/","section":"publication","summary":"One of the most dramatic large-scale features in the ocean is the seasonal greening of the North Atlantic in spring and summer due to the accumulation of phytoplankton biomass in the surface layer. In 1953, Harald Ulrik Sverdrup hypothesized a now canonical mechanism for the development and timing of phytoplankton blooms in the North Atlantic. Over the next 60 years, Sverdrup’s Critical Depth Hypothesis spurred progress in understanding of bloom dynamics and offered a valuable theoretical framework on which to build. In reviewing 60 years of literature, the authors trace the development of modern bloom initiation hypotheses, highlighting three case studies that illuminate the complexity, including both catalysts and impediments, of scientific progress in the wake of Sverdrup’s hypothesis. Most notably, these cases demonstrate that the evolution of our understanding of phytoplankton blooms was paced by access not only to technology but also to concurrent insights from several disciplines. This exploration of the trajectories and successes in bloom studies highlights the need for expanding interdisciplinary collaborations to address the complexity of phytoplankton bloom dynamics.","tags":["critical depth hypothesis","Sverdrup","blooms","phytoplankton"],"title":"Sixty Years of Sverdrup: A Retrospective of Progress in the Study of Phytoplankton Blooms","type":"publication"},{"authors":["**Harriet Alexander**","Bethany D Jenkins","Tatiana A Rynearson","Mak A Saito","Melissa L Mercier","Sonya T Dyhrman"],"categories":null,"content":"","date":1325394000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325394000,"objectID":"5ba5988ed4975123927df4c300915d1b","permalink":"https://alexanderlabwhoi.github.io/publication/alexander2012/","publishdate":"2012-01-01T00:00:00-05:00","relpermalink":"/publication/alexander2012/","section":"publication","summary":"Genes that are constitutively expressed across multiple environmental stimuli are crucial to quantifying differentially expressed genes, particularly when employing quantitative reverse transcriptase polymerase chain reaction (RT-qPCR) assays. However, the identification of these potential reference genes in non-model organisms is challenging and is often guided by expression patterns in distantly related organisms. Here, transcriptome datasets from the diatom *Thalassiosira pseudonana* grown under replete, phosphorus-limited, iron-limited, and phosphorus and iron co-limited nutrient regimes were analyzed through literature-based searches for homologous reference genes, *k*-means clustering, and analysis of sequence counts (ASC) to identify putative reference genes. A total of 9759 genes were identified and screened for stable expression. Literature-based searches surveyed 18 generally accepted reference genes, revealing 101 homologs in *T. pseudonana* with variable expression and a wide range of mean tags per million. *k*-means analysis parsed the whole transcriptome into 15 clusters. The two most stable clusters contained 709 genes, but still had distinct patterns in expression. ASC analyses identified 179 genes that were stably expressed (posterior probability ","tags":["diatom","housekeeping genes","phytoplankton","reference gene","relative gene expression","rt-qpcr","Thalassiosira pseudonana","transcriptomics"],"title":"Identifying reference genes with stable expression from high throughput sequence data","type":"publication"},{"authors":["Sonya T Dyhrman","Bethany D Jenkins","Tatiana A Rynearson","Mak A Saito","Melissa L Mercier","**Harriet Alexander**","Leann P Whitney","Andrea Drzewianowski","Vladimir V Bulygin","Erin M Bertrand","Zhijin Wu","Claudia Benitez-Nelson","Abigail Heithoff"],"categories":null,"content":"","date":1325394000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325394000,"objectID":"909480154ecfe9cca6ea2b15f70f70a9","permalink":"https://alexanderlabwhoi.github.io/publication/dyhrman2012/","publishdate":"2012-01-01T00:00:00-05:00","relpermalink":"/publication/dyhrman2012/","section":"publication","summary":"Phosphorus (P) is a critical driver of phytoplankton growth and ecosystem function in the ocean. Diatoms are an abundant class of marine phytoplankton that are responsible for significant amounts of primary production. With the control they exert on the oceanic carbon cycle, there have been a number of studies focused on how diatoms respond to limiting macro and micronutrients such as iron and nitrogen. However, diatom physiological responses to P deficiency are poorly understood. Here, we couple deep sequencing of transcript tags and quantitative proteomics to analyze the diatom *Thalassiosira pseudonana* grown under P-replete and P-deficient conditions. A total of 318 transcripts were differentially regulated with a false discovery rate of ","tags":["diatom","phosphorus","proteomics","stress","transcriptomics","Thalassiosira pseudonana","gene expression"],"title":"The transcriptome and proteome of the diatom Thalassiosira pseudonana reveal a diverse phosphorus stress response","type":"publication"}]